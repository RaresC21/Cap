{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gurobi, JuMP\n",
    "using Statistics, Random, Distributions\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2021-07-23\n"
     ]
    }
   ],
   "source": [
    "gurobi_env = Gurobi.Env();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_noise (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add_noise(c_observed, noise)\n",
    "    n = size(c_observed)[1]\n",
    "    m = size(c_observed)[2]\n",
    "    c = zeros(n, m)\n",
    "    for j = 1:n\n",
    "        for i = 1:m\n",
    "            epsilon = (1 - noise) + 2 * noise*rand()\n",
    "            c[j, i] = c_observed[j, i] * epsilon\n",
    "        end\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective_fun (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function objective_fun(c,w)\n",
    "#     return sum(c .* w .* w)\n",
    "     return sum(c .* w)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Local.\n",
      "┌ Warning: `Grid` is deprecated, use `grid` instead.\n",
      "│   caller = convert_grid_to_list(::Int64, ::Int64) at shortest_path_oracle.jl:95\n",
      "└ @ Main /home/rares/Desktop/spo/Cap/shortest_path_oracle.jl:95\n"
     ]
    }
   ],
   "source": [
    "include(\"./local.jl\")\n",
    "include(\"shortest_path_oracle.jl\")\n",
    "include(\"util.jl\")\n",
    "import .Local\n",
    "\n",
    "loss_only = true\n",
    "\n",
    "grid_dim = 5\n",
    "p_features = 10\n",
    "n_train = 100\n",
    "holdout_percent = 0.25\n",
    "n_holdout = round(Int, holdout_percent*n_train) \n",
    "n_test = 1000\n",
    "\n",
    "sources, destinations = convert_grid_to_list(grid_dim, grid_dim)\n",
    "sp_oracle = sp_flow_jump_setup_mine(sources, destinations, 1, grid_dim^2, gurobiEnv = gurobi_env)\n",
    "\n",
    "d_feasibleregion = length(sources)\n",
    "B_true = rand(Bernoulli(0.5), d_feasibleregion, p_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function solve_lp(c)\n",
    "    m = sp_oracle(c)\n",
    "    MOI.set(m, MOI.Silent(), true)\n",
    "    @objective(m, Min, objective_fun(c, m[:w]))\n",
    "    optimize!(m)\n",
    "    return value.(m[:w])\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Local.\n"
     ]
    }
   ],
   "source": [
    "include(\"./local.jl\")\n",
    "include(\"shortest_path_oracle.jl\")\n",
    "include(\"util.jl\")\n",
    "import .Local\n",
    "\n",
    "n_train = 800\n",
    "\n",
    "# noise = 0.0\n",
    "(X_train, c_train_original) = generate_poly_kernel_data_simple(B_true, n_train, 1, 0)\n",
    "(X_validation, c_validation) = generate_poly_kernel_data_simple(B_true, n_holdout, 1, 0)\n",
    "(X_test, c_test) = generate_poly_kernel_data_simple(B_true, n_test, 1, 0)\n",
    "\n",
    "# n_sigmoid = 5dt\n",
    "# B_true_1 = rand(Bernoulli(0.25), n_sigmoid, p_features)\n",
    "# B_true_2 = rand(Bernoulli(0.5), d_feasibleregion, n_sigmoid)\n",
    "\n",
    "# (X_train, c_train_original) = sigmoid_data(B_true_1, B_true_2, p_features, n_train)\n",
    "# (X_validation, c_validation) = sigmoid_data(B_true_1, B_true_2, p_features, n_holdout)\n",
    "# (X_test, c_test) = sigmoid_data(B_true_1, B_true_2, p_features, n_test)\n",
    "\n",
    "c_train = c_train_original;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "0.2\n",
      "0.25\n",
      "0.3\n",
      "0.35\n",
      "0.4\n",
      "0.45\n",
      "0.5\n",
      "0.55\n",
      "0.6\n",
      "0.65\n",
      "0.7\n",
      "0.75\n",
      "0.8\n",
      "0.85\n",
      "0.9\n",
      "0.95\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# for noise in 0.0:0.05:1.0\n",
    "#     linear = string(noise)\n",
    "# #     println(linear)\n",
    "    \n",
    "#     c_train = add_noise(c_train_original, noise);\n",
    "\n",
    "#     writedlm(string(\"sp/X_train\", linear, \".csv\"),  X_train, ',')\n",
    "#     writedlm(string(\"sp/X_validation\", linear, \".csv\"),  X_validation, ',')\n",
    "#     writedlm(string(\"sp/X_test\", linear, \".csv\"),  X_test, ',')\n",
    "\n",
    "#     writedlm(string(\"sp/c_train\", linear, \".csv\"),  c_train, ',')\n",
    "#     writedlm(string(\"sp/c_validation\", linear, \".csv\"), c_validation, ',')\n",
    "#     writedlm(string(\"sp/c_test\", linear, \".csv\"),  c_test, ',')\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_data (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DelimitedFiles\n",
    "\n",
    "function get_data(noise)\n",
    "    linear = string(noise)\n",
    "    X_train  = readdlm(string(\"sp/X_train\", linear, \".csv\"),  ',')\n",
    "    X_validation = readdlm(string(\"sp/X_validation\", linear, \".csv\"),  ',')\n",
    "    X_test = readdlm(string(\"sp/X_test\", linear, \".csv\"),  ',')\n",
    "\n",
    "    c_train = readdlm(string(\"sp/c_train\", linear, \".csv\"),  ',')\n",
    "    c_validation = readdlm(string(\"sp/c_validation\", linear, \".csv\"), ',')\n",
    "    c_test = readdlm(string(\"sp/c_test\", linear, \".csv\"),  ',');\n",
    "\n",
    "    return X_train, c_train, X_validation, c_validation, X_test, c_test\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = 0.0\n",
    "# c_train = add_noise(c_train_original, noise);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `with_optimizer` is deprecated, replace `with_optimizer(Gurobi.Optimizer, env)` by `() -> Gurobi.Optimizer(env)`.\n",
      "│   caller = ip:0x0\n",
      "└ @ Core :-1\n",
      "┌ Warning: `with_optimizer` is deprecated, replace `with_optimizer(Ipopt.Optimizer)` by `Ipopt.Optimizer`.\n",
      "│   caller = #with_optimizer#6(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Type, ::Gurobi.Env) at JuMP.jl:122\n",
      "└ @ JuMP /home/rares/.julia/packages/JuMP/Sp4sR/src/JuMP.jl:122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 2\n",
      "Run: 3\n",
      "Run: 4\n",
      "Run: 5\n",
      "Run: 6\n",
      "Run: 7\n",
      "Run: 8\n",
      "Run: 9\n",
      "Run: 10\n",
      "Run: 11\n",
      "Run: 12\n",
      "Run: 13\n",
      "Run: 14\n",
      "Run: 15\n",
      "Run: 16\n",
      "Run: 17\n",
      "Run: 18\n",
      "Run: 19\n",
      "Run: 20\n",
      "Run: 21\n",
      "Run: 22\n",
      "Run: 23\n",
      "Run: 24\n",
      "Run: 25\n",
      "Run: 26\n",
      "Run: 27\n",
      "Run: 28\n",
      "Run: 29\n",
      "Run: 30\n",
      "Run: 31\n",
      "Run: 32\n",
      "Run: 33\n",
      "Run: 34\n",
      "Run: 35\n",
      "Run: 36\n",
      "Run: 37\n",
      "Run: 38\n",
      "Run: 39\n",
      "Run: 40\n",
      "Run: 41\n",
      "Run: 42\n",
      "Run: 43\n",
      "Run: 44\n",
      "Run: 45\n",
      "Run: 46\n",
      "Run: 47\n",
      "Run: 48\n",
      "Run: 49\n",
      "Run: 50\n",
      "Run: 51\n",
      "Run: 52\n",
      "Run: 53\n",
      "Run: 54\n",
      "Run: 55\n",
      "Run: 56\n",
      "Run: 57\n",
      "Run: 58\n",
      "Run: 59\n",
      "Run: 60\n",
      "Run: 61\n",
      "Run: 62\n",
      "Run: 63\n",
      "Run: 64\n",
      "Run: 65\n",
      "Run: 66\n",
      "Run: 67\n",
      "Run: 68\n",
      "Run: 69\n",
      "Run: 70\n",
      "Run: 71\n",
      "Run: 72\n",
      "Run: 73\n",
      "Run: 74\n",
      "Run: 75\n",
      "Run: 76\n",
      "Run: 77\n",
      "Run: 78\n",
      "Run: 79\n",
      "Run: 80\n",
      "Run: 81\n",
      "Run: 82\n",
      "Run: 83\n",
      "Run: 84\n",
      "Run: 85\n",
      "Run: 86\n",
      "Run: 87\n",
      "Run: 88\n",
      "Run: 89\n",
      "Run: 90\n",
      "Run: 91\n",
      "Run: 92\n",
      "Run: 93\n",
      "Run: 94\n",
      "Run: 95\n",
      "Run: 96\n",
      "Run: 97\n",
      "Run: 98\n",
      "Run: 99\n",
      "Run: 100\n",
      "Run: 101\n",
      "Run: 102\n",
      "Run: 103\n",
      "Run: 104\n",
      "Run: 105\n",
      "Run: 106\n",
      "Run: 107\n",
      "Run: 108\n",
      "Run: 109\n",
      "Run: 110\n",
      "Run: 111\n",
      "Run: 112\n",
      "Run: 113\n",
      "Run: 114\n",
      "Run: 115\n",
      "Run: 116\n",
      "Run: 117\n",
      "Run: 118\n",
      "Run: 119\n",
      "Run: 120\n",
      "Run: 121\n",
      "Run: 122\n",
      "Run: 123\n",
      "Run: 124\n",
      "Run: 125\n",
      "Run: 126\n",
      "Run: 127\n",
      "Run: 128\n",
      "Run: 129\n",
      "Run: 130\n",
      "Run: 131\n",
      "Run: 132\n",
      "Run: 133\n",
      "Run: 134\n",
      "Run: 135\n",
      "Run: 136\n",
      "Run: 137\n",
      "Run: 138\n",
      "Run: 139\n",
      "Run: 140\n",
      "Run: 141\n",
      "Run: 142\n",
      "Run: 143\n",
      "Run: 144\n",
      "Run: 145\n",
      "Run: 146\n",
      "Run: 147\n",
      "Run: 148\n",
      "Run: 149\n",
      "Run: 150\n",
      "Run: 151\n",
      "Run: 152\n",
      "Run: 153\n",
      "Run: 154\n",
      "Run: 155\n",
      "Run: 156\n",
      "Run: 157\n",
      "Run: 158\n",
      "Run: 159\n",
      "Run: 160\n",
      "Run: 161\n",
      "Run: 162\n",
      "Run: 163\n",
      "Run: 164\n",
      "Run: 165\n",
      "Run: 166\n",
      "Run: 167\n",
      "Run: 168\n",
      "Run: 169\n",
      "Run: 170\n",
      "Run: 171\n",
      "Run: 172\n",
      "Run: 173\n",
      "Run: 174\n",
      "Run: 175\n",
      "Run: 176\n",
      "Run: 177\n",
      "Run: 178\n",
      "Run: 179\n",
      "Run: 180\n",
      "Run: 181\n",
      "Run: 182\n",
      "Run: 183\n",
      "Run: 184\n",
      "Run: 185\n",
      "Run: 186\n",
      "Run: 187\n",
      "Run: 188\n",
      "Run: 189\n",
      "Run: 190\n",
      "Run: 191\n",
      "Run: 192\n",
      "Run: 193\n",
      "Run: 194\n",
      "Run: 195\n",
      "Run: 196\n",
      "Run: 197\n",
      "Run: 198\n",
      "Run: 199\n",
      "Run: 200\n",
      "Run: 201\n",
      "Run: 202\n",
      "Run: 203\n",
      "Run: 204\n",
      "Run: 205\n",
      "Run: 206\n",
      "Run: 207\n",
      "Run: 208\n",
      "Run: 209\n",
      "Run: 210\n",
      "Run: 211\n",
      "Run: 212\n",
      "Run: 213\n",
      "Run: 214\n",
      "Run: 215\n",
      "Run: 216\n",
      "Run: 217\n",
      "Run: 218\n",
      "Run: 219\n",
      "Run: 220\n",
      "Run: 221\n",
      "Run: 222\n",
      "Run: 223\n",
      "Run: 224\n",
      "Run: 225\n",
      "Run: 226\n",
      "Run: 227\n",
      "Run: 228\n",
      "Run: 229\n",
      "Run: 230\n",
      "Run: 231\n",
      "Run: 232\n",
      "Run: 233\n",
      "Run: 234\n",
      "Run: 235\n",
      "Run: 236\n",
      "Run: 237\n",
      "Run: 238\n",
      "Run: 239\n",
      "Run: 240\n",
      "Run: 241\n",
      "Run: 242\n",
      "Run: 243\n",
      "Run: 244\n",
      "Run: 245\n",
      "Run: 246\n",
      "Run: 247\n",
      "Run: 248\n",
      "Run: 249\n",
      "Run: 250\n",
      "Run: 251\n",
      "Run: 252\n",
      "Run: 253\n",
      "Run: 254\n",
      "Run: 255\n",
      "Run: 256\n",
      "Run: 257\n",
      "Run: 258\n",
      "Run: 259\n",
      "Run: 260\n",
      "Run: 261\n",
      "Run: 262\n",
      "Run: 263\n",
      "Run: 264\n",
      "Run: 265\n",
      "Run: 266\n",
      "Run: 267\n",
      "Run: 268\n",
      "Run: 269\n",
      "Run: 270\n",
      "Run: 271\n",
      "Run: 272\n",
      "Run: 273\n",
      "Run: 274\n",
      "Run: 275\n",
      "Run: 276\n",
      "Run: 277\n",
      "Run: 278\n",
      "Run: 279\n",
      "Run: 280\n",
      "Run: 281\n",
      "Run: 282\n",
      "Run: 283\n",
      "Run: 284\n",
      "Run: 285\n",
      "Run: 286\n",
      "Run: 287\n",
      "Run: 288\n",
      "Run: 289\n",
      "Run: 290\n",
      "Run: 291\n",
      "Run: 292\n",
      "Run: 293\n",
      "Run: 294\n",
      "Run: 295\n",
      "Run: 296\n",
      "Run: 297\n",
      "Run: 298\n",
      "Run: 299\n",
      "Run: 300\n",
      "Run: 301\n",
      "Run: 302\n",
      "Run: 303\n",
      "Run: 304\n",
      "Run: 305\n",
      "Run: 306\n",
      "Run: 307\n",
      "Run: 308\n",
      "Run: 309\n",
      "Run: 310\n",
      "Run: 311\n",
      "Run: 312\n",
      "Run: 313\n",
      "Run: 314\n",
      "Run: 315\n",
      "Run: 316\n",
      "Run: 317\n",
      "Run: 318\n",
      "Run: 319\n",
      "Run: 320\n",
      "Run: 321\n",
      "Run: 322\n",
      "Run: 323\n",
      "Run: 324\n",
      "Run: 325\n",
      "Run: 326\n",
      "Run: 327\n",
      "Run: 328\n",
      "Run: 329\n",
      "Run: 330\n",
      "Run: 331\n",
      "Run: 332\n",
      "Run: 333\n",
      "Run: 334\n",
      "Run: 335\n",
      "Run: 336\n",
      "Run: 337\n",
      "Run: 338\n",
      "Run: 339\n",
      "Run: 340\n",
      "Run: 341\n",
      "Run: 342\n",
      "Run: 343\n",
      "Run: 344\n",
      "Run: 345\n",
      "Run: 346\n",
      "Run: 347\n",
      "Run: 348\n",
      "Run: 349\n",
      "Run: 350\n",
      "Run: 351\n",
      "Run: 352\n",
      "Run: 353\n",
      "Run: 354\n",
      "Run: 355\n",
      "Run: 356\n",
      "Run: 357\n",
      "Run: 358\n",
      "Run: 359\n",
      "Run: 360\n",
      "Run: 361\n",
      "Run: 362\n",
      "Run: 363\n",
      "Run: 364\n",
      "Run: 365\n",
      "Run: 366\n",
      "Run: 367\n",
      "Run: 368\n",
      "Run: 369\n",
      "Run: 370\n",
      "Run: 371\n",
      "Run: 372\n",
      "Run: 373\n",
      "Run: 374\n",
      "Run: 375\n",
      "Run: 376\n",
      "Run: 377\n",
      "Run: 378\n",
      "Run: 379\n",
      "Run: 380\n",
      "Run: 381\n",
      "Run: 382\n",
      "Run: 383\n",
      "Run: 384\n",
      "Run: 385\n",
      "Run: 386\n",
      "Run: 387\n",
      "Run: 388\n",
      "Run: 389\n",
      "Run: 390\n",
      "Run: 391\n",
      "Run: 392\n",
      "Run: 393\n",
      "Run: 394\n",
      "Run: 395\n",
      "Run: 396\n",
      "Run: 397\n",
      "Run: 398\n",
      "Run: 399\n",
      "Run: 400\n",
      "Run: 401\n",
      "Run: 402\n",
      "Run: 403\n",
      "Run: 404\n",
      "Run: 405\n",
      "Run: 406\n",
      "Run: 407\n",
      "Run: 408\n",
      "Run: 409\n",
      "Run: 410\n",
      "Run: 411\n",
      "Run: 412\n",
      "Run: 413\n",
      "Run: 414\n",
      "Run: 415\n",
      "Run: 416\n",
      "Run: 417\n",
      "Run: 418\n",
      "Run: 419\n",
      "Run: 420\n",
      "Run: 421\n",
      "Run: 422\n",
      "Run: 423\n",
      "Run: 424\n",
      "Run: 425\n",
      "Run: 426\n",
      "Run: 427\n",
      "Run: 428\n",
      "Run: 429\n",
      "Run: 430\n",
      "Run: 431\n",
      "Run: 432\n",
      "Run: 433\n",
      "Run: 434\n",
      "Run: 435\n",
      "Run: 436\n",
      "Run: 437\n",
      "Run: 438\n",
      "Run: 439\n",
      "Run: 440\n",
      "Run: 441\n",
      "Run: 442\n",
      "Run: 443\n",
      "Run: 444\n",
      "Run: 445\n",
      "Run: 446\n",
      "Run: 447\n",
      "Run: 448\n",
      "Run: 449\n",
      "Run: 450\n",
      "Run: 451\n",
      "Run: 452\n",
      "Run: 453\n",
      "Run: 454\n",
      "Run: 455\n",
      "Run: 456\n",
      "Run: 457\n",
      "Run: 458\n",
      "Run: 459\n",
      "Run: 460\n",
      "Run: 461\n",
      "Run: 462\n",
      "Run: 463\n",
      "Run: 464\n",
      "Run: 465\n",
      "Run: 466\n",
      "Run: 467\n",
      "Run: 468\n",
      "Run: 469\n",
      "Run: 470\n",
      "Run: 471\n",
      "Run: 472\n",
      "Run: 473\n",
      "Run: 474\n",
      "Run: 475\n",
      "Run: 476\n",
      "Run: 477\n",
      "Run: 478\n",
      "Run: 479\n",
      "Run: 480\n",
      "Run: 481\n",
      "Run: 482\n",
      "Run: 483\n",
      "Run: 484\n",
      "Run: 485\n",
      "Run: 486\n",
      "Run: 487\n",
      "Run: 488\n",
      "Run: 489\n",
      "Run: 490\n",
      "Run: 491\n",
      "Run: 492\n",
      "Run: 493\n",
      "Run: 494\n",
      "Run: 495\n",
      "Run: 496\n",
      "Run: 497\n",
      "Run: 498\n",
      "Run: 499\n",
      "Run: 500\n",
      "Run: 501\n",
      "Run: 502\n",
      "Run: 503\n",
      "Run: 504\n",
      "Run: 505\n",
      "Run: 506\n",
      "Run: 507\n",
      "Run: 508\n",
      "Run: 509\n",
      "Run: 510\n",
      "Run: 511\n",
      "Run: 512\n",
      "Run: 513\n",
      "Run: 514\n",
      "Run: 515\n",
      "Run: 516\n",
      "Run: 517\n",
      "Run: 518\n",
      "Run: 519\n",
      "Run: 520\n",
      "Run: 521\n",
      "Run: 522\n",
      "Run: 523\n",
      "Run: 524\n",
      "Run: 525\n",
      "Run: 526\n",
      "Run: 527\n",
      "Run: 528\n",
      "Run: 529\n",
      "Run: 530\n",
      "Run: 531\n",
      "Run: 532\n",
      "Run: 533\n",
      "Run: 534\n",
      "Run: 535\n",
      "Run: 536\n",
      "Run: 537\n",
      "Run: 538\n",
      "Run: 539\n",
      "Run: 540\n",
      "Run: 541\n",
      "Run: 542\n",
      "Run: 543\n",
      "Run: 544\n",
      "Run: 545\n",
      "Run: 546\n",
      "Run: 547\n",
      "Run: 548\n",
      "Run: 549\n",
      "Run: 550\n",
      "Run: 551\n",
      "Run: 552\n",
      "Run: 553\n",
      "Run: 554\n",
      "Run: 555\n",
      "Run: 556\n",
      "Run: 557\n",
      "Run: 558\n",
      "Run: 559\n",
      "Run: 560\n",
      "Run: 561\n",
      "Run: 562\n",
      "Run: 563\n",
      "Run: 564\n",
      "Run: 565\n",
      "Run: 566\n",
      "Run: 567\n",
      "Run: 568\n",
      "Run: 569\n",
      "Run: 570\n",
      "Run: 571\n",
      "Run: 572\n",
      "Run: 573\n",
      "Run: 574\n",
      "Run: 575\n",
      "Run: 576\n",
      "Run: 577\n",
      "Run: 578\n",
      "Run: 579\n",
      "Run: 580\n",
      "Run: 581\n",
      "Run: 582\n",
      "Run: 583\n",
      "Run: 584\n",
      "Run: 585\n",
      "Run: 586\n",
      "Run: 587\n",
      "Run: 588\n",
      "Run: 589\n",
      "Run: 590\n",
      "Run: 591\n",
      "Run: 592\n",
      "Run: 593\n",
      "Run: 594\n",
      "Run: 595\n",
      "Run: 596\n",
      "Run: 597\n",
      "Run: 598\n",
      "Run: 599\n",
      "Run: 600\n",
      "Run: 601\n",
      "Run: 602\n",
      "Run: 603\n",
      "Run: 604\n",
      "Run: 605\n",
      "Run: 606\n",
      "Run: 607\n",
      "Run: 608\n",
      "Run: 609\n",
      "Run: 610\n",
      "Run: 611\n",
      "Run: 612\n",
      "Run: 613\n",
      "Run: 614\n",
      "Run: 615\n",
      "Run: 616\n",
      "Run: 617\n",
      "Run: 618\n",
      "Run: 619\n",
      "Run: 620\n",
      "Run: 621\n",
      "Run: 622\n",
      "Run: 623\n",
      "Run: 624\n",
      "Run: 625\n",
      "Run: 626\n",
      "Run: 627\n",
      "Run: 628\n",
      "Run: 629\n",
      "Run: 630\n",
      "Run: 631\n",
      "Run: 632\n",
      "Run: 633\n",
      "Run: 634\n",
      "Run: 635\n",
      "Run: 636\n",
      "Run: 637\n",
      "Run: 638\n",
      "Run: 639\n",
      "Run: 640\n",
      "Run: 641\n",
      "Run: 642\n",
      "Run: 643\n",
      "Run: 644\n",
      "Run: 645\n",
      "Run: 646\n",
      "Run: 647\n",
      "Run: 648\n",
      "Run: 649\n",
      "Run: 650\n",
      "Run: 651\n",
      "Run: 652\n",
      "Run: 653\n",
      "Run: 654\n",
      "Run: 655\n",
      "Run: 656\n",
      "Run: 657\n",
      "Run: 658\n",
      "Run: 659\n",
      "Run: 660\n",
      "Run: 661\n",
      "Run: 662\n",
      "Run: 663\n",
      "Run: 664\n",
      "Run: 665\n",
      "Run: 666\n",
      "Run: 667\n",
      "Run: 668\n",
      "Run: 669\n",
      "Run: 670\n",
      "Run: 671\n",
      "Run: 672\n",
      "Run: 673\n",
      "Run: 674\n",
      "Run: 675\n",
      "Run: 676\n",
      "Run: 677\n",
      "Run: 678\n",
      "Run: 679\n",
      "Run: 680\n",
      "Run: 681\n",
      "Run: 682\n",
      "Run: 683\n",
      "Run: 684\n",
      "Run: 685\n",
      "Run: 686\n",
      "Run: 687\n",
      "Run: 688\n",
      "Run: 689\n",
      "Run: 690\n",
      "Run: 691\n",
      "Run: 692\n",
      "Run: 693\n",
      "Run: 694\n",
      "Run: 695\n",
      "Run: 696\n",
      "Run: 697\n",
      "Run: 698\n",
      "Run: 699\n",
      "Run: 700\n",
      "Run: 701\n",
      "Run: 702\n",
      "Run: 703\n",
      "Run: 704\n",
      "Run: 705\n",
      "Run: 706\n",
      "Run: 707\n",
      "Run: 708\n",
      "Run: 709\n",
      "Run: 710\n",
      "Run: 711\n",
      "Run: 712\n",
      "Run: 713\n",
      "Run: 714\n",
      "Run: 715\n",
      "Run: 716\n",
      "Run: 717\n",
      "Run: 718\n",
      "Run: 719\n",
      "Run: 720\n",
      "Run: 721\n",
      "Run: 722\n",
      "Run: 723\n",
      "Run: 724\n",
      "Run: 725\n",
      "Run: 726\n",
      "Run: 727\n",
      "Run: 728\n",
      "Run: 729\n",
      "Run: 730\n",
      "Run: 731\n",
      "Run: 732\n",
      "Run: 733\n",
      "Run: 734\n",
      "Run: 735\n",
      "Run: 736\n",
      "Run: 737\n",
      "Run: 738\n",
      "Run: 739\n",
      "Run: 740\n",
      "Run: 741\n",
      "Run: 742\n",
      "Run: 743\n",
      "Run: 744\n",
      "Run: 745\n",
      "Run: 746\n",
      "Run: 747\n",
      "Run: 748\n",
      "Run: 749\n",
      "Run: 750\n",
      "Run: 751\n",
      "Run: 752\n",
      "Run: 753\n",
      "Run: 754\n",
      "Run: 755\n",
      "Run: 756\n",
      "Run: 757\n",
      "Run: 758\n",
      "Run: 759\n",
      "Run: 760\n",
      "Run: 761\n",
      "Run: 762\n",
      "Run: 763\n",
      "Run: 764\n",
      "Run: 765\n",
      "Run: 766\n",
      "Run: 767\n",
      "Run: 768\n",
      "Run: 769\n",
      "Run: 770\n",
      "Run: 771\n",
      "Run: 772\n",
      "Run: 773\n",
      "Run: 774\n",
      "Run: 775\n",
      "Run: 776\n",
      "Run: 777\n",
      "Run: 778\n",
      "Run: 779\n",
      "Run: 780\n",
      "Run: 781\n",
      "Run: 782\n",
      "Run: 783\n",
      "Run: 784\n",
      "Run: 785\n",
      "Run: 786\n",
      "Run: 787\n",
      "Run: 788\n",
      "Run: 789\n",
      "Run: 790\n",
      "Run: 791\n",
      "Run: 792\n",
      "Run: 793\n",
      "Run: 794\n",
      "Run: 795\n",
      "Run: 796\n",
      "Run: 797\n",
      "Run: 798\n",
      "Run: 799\n",
      "Run: 800\n",
      "Run: 801\n",
      "Run: 802\n",
      "Run: 803\n",
      "Run: 804\n",
      "Run: 805\n",
      "Run: 806\n",
      "Run: 807\n",
      "Run: 808\n",
      "Run: 809\n",
      "Run: 810\n",
      "Run: 811\n",
      "Run: 812\n",
      "Run: 813\n",
      "Run: 814\n",
      "Run: 815\n",
      "Run: 816\n",
      "Run: 817\n",
      "Run: 818\n",
      "Run: 819\n",
      "Run: 820\n",
      "Run: 821\n",
      "Run: 822\n",
      "Run: 823\n",
      "Run: 824\n",
      "Run: 825\n",
      "Run: 826\n",
      "Run: 827\n",
      "Run: 828\n",
      "Run: 829\n",
      "Run: 830\n",
      "Run: 831\n",
      "Run: 832\n",
      "Run: 833\n",
      "Run: 834\n",
      "Run: 835\n",
      "Run: 836\n",
      "Run: 837\n",
      "Run: 838\n",
      "Run: 839\n",
      "Run: 840\n",
      "Run: 841\n",
      "Run: 842\n",
      "Run: 843\n",
      "Run: 844\n",
      "Run: 845\n",
      "Run: 846\n",
      "Run: 847\n",
      "Run: 848\n",
      "Run: 849\n",
      "Run: 850\n",
      "Run: 851\n",
      "Run: 852\n",
      "Run: 853\n",
      "Run: 854\n",
      "Run: 855\n",
      "Run: 856\n",
      "Run: 857\n",
      "Run: 858\n",
      "Run: 859\n",
      "Run: 860\n",
      "Run: 861\n",
      "Run: 862\n",
      "Run: 863\n",
      "Run: 864\n",
      "Run: 865\n",
      "Run: 866\n",
      "Run: 867\n",
      "Run: 868\n",
      "Run: 869\n",
      "Run: 870\n",
      "Run: 871\n",
      "Run: 872\n",
      "Run: 873\n",
      "Run: 874\n",
      "Run: 875\n",
      "Run: 876\n",
      "Run: 877\n",
      "Run: 878\n",
      "Run: 879\n",
      "Run: 880\n",
      "Run: 881\n",
      "Run: 882\n",
      "Run: 883\n",
      "Run: 884\n",
      "Run: 885\n",
      "Run: 886\n",
      "Run: 887\n",
      "Run: 888\n",
      "Run: 889\n",
      "Run: 890\n",
      "Run: 891\n",
      "Run: 892\n",
      "Run: 893\n",
      "Run: 894\n",
      "Run: 895\n",
      "Run: 896\n",
      "Run: 897\n",
      "Run: 898\n",
      "Run: 899\n",
      "Run: 900\n",
      "Run: 901\n",
      "Run: 902\n",
      "Run: 903\n",
      "Run: 904\n",
      "Run: 905\n",
      "Run: 906\n",
      "Run: 907\n",
      "Run: 908\n",
      "Run: 909\n",
      "Run: 910\n",
      "Run: 911\n",
      "Run: 912\n",
      "Run: 913\n",
      "Run: 914\n",
      "Run: 915\n",
      "Run: 916\n",
      "Run: 917\n",
      "Run: 918\n",
      "Run: 919\n",
      "Run: 920\n",
      "Run: 921\n",
      "Run: 922\n",
      "Run: 923\n",
      "Run: 924\n",
      "Run: 925\n",
      "Run: 926\n",
      "Run: 927\n",
      "Run: 928\n",
      "Run: 929\n",
      "Run: 930\n",
      "Run: 931\n",
      "Run: 932\n",
      "Run: 933\n",
      "Run: 934\n",
      "Run: 935\n",
      "Run: 936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 937\n",
      "Run: 938\n",
      "Run: 939\n",
      "Run: 940\n",
      "Run: 941\n",
      "Run: 942\n",
      "Run: 943\n",
      "Run: 944\n",
      "Run: 945\n",
      "Run: 946\n",
      "Run: 947\n",
      "Run: 948\n",
      "Run: 949\n",
      "Run: 950\n",
      "Run: 951\n",
      "Run: 952\n",
      "Run: 953\n",
      "Run: 954\n",
      "Run: 955\n",
      "Run: 956\n",
      "Run: 957\n",
      "Run: 958\n",
      "Run: 959\n",
      "Run: 960\n",
      "Run: 961\n",
      "Run: 962\n",
      "Run: 963\n",
      "Run: 964\n",
      "Run: 965\n",
      "Run: 966\n",
      "Run: 967\n",
      "Run: 968\n",
      "Run: 969\n",
      "Run: 970\n",
      "Run: 971\n",
      "Run: 972\n",
      "Run: 973\n",
      "Run: 974\n",
      "Run: 975\n",
      "Run: 976\n",
      "Run: 977\n",
      "Run: 978\n",
      "Run: 979\n",
      "Run: 980\n",
      "Run: 981\n",
      "Run: 982\n",
      "Run: 983\n",
      "Run: 984\n",
      "Run: 985\n",
      "Run: 986\n",
      "Run: 987\n",
      "Run: 988\n",
      "Run: 989\n",
      "Run: 990\n",
      "Run: 991\n",
      "Run: 992\n",
      "Run: 993\n",
      "Run: 994\n",
      "Run: 995\n",
      "Run: 996\n",
      "Run: 997\n",
      "Run: 998\n",
      "Run: 999\n",
      "Run: 1000\n",
      "Run: 1\n",
      "Run: 2\n",
      "Run: 3\n",
      "Run: 4\n",
      "Run: 5\n",
      "Run: 6\n",
      "Run: 7\n",
      "Run: 8\n",
      "Run: 9\n",
      "Run: 10\n",
      "Run: 11\n",
      "Run: 12\n",
      "Run: 13\n",
      "Run: 14\n",
      "Run: 15\n",
      "Run: 16\n",
      "Run: 17\n",
      "Run: 18\n",
      "Run: 19\n",
      "Run: 20\n",
      "Run: 21\n",
      "Run: 22\n",
      "Run: 23\n",
      "Run: 24\n",
      "Run: 25\n"
     ]
    }
   ],
   "source": [
    "true_weights = Vector{Vector{Float64}}()\n",
    "validation_weights =  Vector{Vector{Float64}}()\n",
    "for i = 1:n_test\n",
    "    println(\"Run: \", i)\n",
    "    true_cost = c_test[:,i]\n",
    "    true_w = solve_lp(true_cost)\n",
    "    push!(true_weights, true_w)    \n",
    "end\n",
    "\n",
    "for i = 1:n_holdout\n",
    "    println(\"Run: \", i)\n",
    "    true_cost = c_validation[:,i]\n",
    "    true_w_ = solve_lp(true_cost)\n",
    "    push!(validation_weights, true_w_)    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Local.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eval_two_stage (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"./local.jl\")\n",
    "import .Local\n",
    "\n",
    "using ScikitLearn\n",
    "@sk_import neighbors: KNeighborsRegressor\n",
    "\n",
    "@sk_import neural_network: MLPRegressor\n",
    "\n",
    "function eval_two_stage(predictor)\n",
    "    all_errors = Vector{Float64}()\n",
    "    nn_all_errors = Vector{Float64}()\n",
    "    true_objs = Vector{Float64}()\n",
    "    my_objs = Vector{Float64}()\n",
    "\n",
    "    adam_loss = 0\n",
    "    nn_adam_loss = 0\n",
    "    trivial_loss = 0 \n",
    "    my_loss = 0\n",
    "    optimum = 0\n",
    "    for i = 1:size(c_test)[2]\n",
    "        x = X_test[:,i]\n",
    "        true_cost = c_test[:,i]\n",
    "\n",
    "        true_obj = objective_fun(true_cost, true_weights[i,:][1])\n",
    "\n",
    "        nn_c = predictor(x)\n",
    "        nn_presc = solve_lp(nn_c)\n",
    "        nn_obj = objective_fun(true_cost, nn_presc)\n",
    "\n",
    "        if nn_obj < true_obj\n",
    "            println(\"NOOOOOOOO- -------------------------\")\n",
    "        end\n",
    "\n",
    "        nn_error = (nn_obj - true_obj) / true_obj\n",
    "#         println(nn_error)\n",
    "\n",
    "        push!(nn_all_errors, nn_error)\n",
    "\n",
    "#         println(\"VALUES: \", mean(nn_all_errors), \" \", median(nn_all_errors))\n",
    "    end;\n",
    "    \n",
    "    return mean(nn_all_errors), nn_all_errors\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_regression_tree (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"./DT.jl\")\n",
    "import .DecisionTree_\n",
    "\n",
    "function eval_regression_tree()\n",
    "    best = 1e5\n",
    "    d = -1\n",
    "    params = 0\n",
    "    for depth = 1:2:30\n",
    "        for samples in 1:9:30\n",
    "            tree = DecisionTree_.train_tree_(X_train', c_train'; depth = depth, min_samples_leaf = samples)\n",
    "            adam_loss, errors = DecisionTree_.evaluate_tree_(X_test', c_test', tree, sp_oracle, objective_fun; loss_only = false)\n",
    "            mm = mean(errors)\n",
    "            if mm < best\n",
    "                best = mm \n",
    "                params = depth, samples\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return best, params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `with_optimizer` is deprecated, replace `with_optimizer(Ipopt.Optimizer)` by `Ipopt.Optimizer`.\n",
      "│   caller = #with_optimizer#6(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Type, ::Gurobi.Env) at JuMP.jl:122\n",
      "└ @ JuMP /home/rares/.julia/packages/JuMP/Sp4sR/src/JuMP.jl:122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4107067879471496, (1, 10))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_regression_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_prescriptive_tree (generic function with 1 method)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function eval_prescriptive_tree(tree, tree_prescription)\n",
    "    tree_bk = Vector{Float64}()\n",
    "    for i = 1:n_test\n",
    "        x = X_test[:,i]\n",
    "        true_cost = c_test[:,i]\n",
    "\n",
    "        true_obj = objective_fun(true_cost, true_weights[i,:][1])\n",
    "\n",
    "        leaf = tree.apply([x])[1]\n",
    "        tree_obj = objective_fun(true_cost, tree_prescription[leaf])\n",
    "        error = (tree_obj - true_obj) / true_obj\n",
    "\n",
    "        push!(tree_bk, error)\n",
    "    end;\n",
    "    return mean(tree_bk)\n",
    "end\n",
    "\n",
    "function get_prescriptive_tree()\n",
    "    best = 100000\n",
    "    params = -1\n",
    "    for depth = 1:2:30\n",
    "        for samples in 1:9:30\n",
    "    \n",
    "            tree = DecisionTree_.train_tree_(X_train', c_train'; depth = depth, min_samples_leaf = samples)\n",
    "\n",
    "            bins = tree.apply(X_train')\n",
    "            tree_region = Dict()\n",
    "            for i = 1:n_train \n",
    "                if haskey(tree_region, bins[i])\n",
    "                    cur = tree_region[bins[i]]  \n",
    "                    push!(cur, i)\n",
    "                else\n",
    "                    cur = [i]\n",
    "                end\n",
    "                tree_region[bins[i]] = cur\n",
    "            end \n",
    "\n",
    "            tree_prescription = Dict()\n",
    "            for (key, vals) in tree_region\n",
    "                m = sp_oracle(zeros(d_feasibleregion))\n",
    "                MOI.set(m, MOI.Silent(), true)\n",
    "\n",
    "                @objective(m, Min, sum( objective_fun(c_train[:,v], m[:w]) for v in vals))\n",
    "\n",
    "                optimize!(m)\n",
    "                tree_prescription[key] = value.(m[:w])\n",
    "            end          \n",
    "           \n",
    "            mm = eval_prescriptive_tree(tree, tree_prescription)\n",
    "            if mm < best \n",
    "                best = mm \n",
    "                params = depth, samples, tree, tree_prescription\n",
    "            end\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    return best, params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4107067879471496, (1, 10, PyObject DecisionTreeRegressor(criterion='mse', max_depth=1, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=10, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best'), Dict{Any,Any}(2=>[1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],1=>[1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0])))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prescriptive_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_least_squares (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least Squares\n",
    "include(\"./least_squares.jl\")\n",
    "import .LS\n",
    "\n",
    "function eval_least_squares()\n",
    "    B = LS.train(X_train, c_train; epochs=200, lr = 1e-2);\n",
    "\n",
    "    adam_loss, ls_errors = LS.evaluate_B(X_test, c_test, B, sp_oracle, objective_fun; loss_only = false)\n",
    "    return mean(ls_errors)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41867768669797467"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "median(ls_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Local.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eval (generic function with 2 methods)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"./local.jl\")\n",
    "import .Local\n",
    "\n",
    "function eval(P, x_data, c_data, true_weights)\n",
    "    all_errors = Vector{Float64}()\n",
    "    n = size(c_data)[2]\n",
    "    total = 0\n",
    "    sum_all = 0\n",
    "#     println(size(c_data))\n",
    "    for i = 1:n\n",
    "        x = x_data[:,i]\n",
    "        true_cost = c_data[:,i]\n",
    "\n",
    "        true_obj = objective_fun(true_cost, true_weights[i,:][1])\n",
    "\n",
    "        my_prescription = Local.predict(P, x; eps = 0)\n",
    "        my_obj = objective_fun(true_cost, my_prescription)\n",
    "\n",
    "#         println(true_obj,  \" \", my_obj, \" \", (my_obj - true_obj) / true_obj)\n",
    "        \n",
    "        total += (my_obj - true_obj)\n",
    "        sum_all += true_obj\n",
    "        error = (my_obj - true_obj) / true_obj\n",
    "        push!(all_errors, error)\n",
    "        if i % 10 == 0\n",
    "            println(mean(all_errors))\n",
    "        end\n",
    "    end;\n",
    "    \n",
    "    println(\"LOSS        : \", total / sum_all)\n",
    "    println(\"MEAN   ERROR: \", mean(all_errors))\n",
    "    println(\"MEDIAN ERROR: \", median(all_errors))\n",
    "    return mean(all_errors), all_errors\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Local.\n",
      "WARNING: redefining constant DecisionTreeClassifier\n",
      "WARNING: redefining constant MLPClassifier\n",
      "WARNING: redefining constant RandomForestClassifier\n",
      "WARNING: redefining constant KNeighborsClassifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cross_validate (generic function with 1 method)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"./local.jl\")\n",
    "\n",
    "using ScikitLearn\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neural_network: MLPClassifier\n",
    "@sk_import ensemble: RandomForestClassifier\n",
    "@sk_import neighbors: KNeighborsClassifier\n",
    "\n",
    "function cross_validate(P, epsilons; model = :nn, widths = [10, 30, 50, 70], test = false, use_validation_data = false)\n",
    "    weights = true_weights\n",
    "    x_data = X_test\n",
    "    c_data = c_test\n",
    "    if use_validation_data\n",
    "        x_data = X_validation\n",
    "        c_data = c_validation\n",
    "        weights = validation_weights\n",
    "    end\n",
    "    \n",
    "    all_error = 0\n",
    "    results = Vector{Any}()\n",
    "    for EPS in epsilons \n",
    "        if !test\n",
    "            Local.init_n(P, EPS)\n",
    "            for i=1:n_train\n",
    "                println(size(P.N[i,:][1]))\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if model == :nn \n",
    "            for w in widths\n",
    "                if !test\n",
    "                    Local.init_models(P, \n",
    "                        MLPClassifier(solver=\"lbfgs\", alpha=1e-5, hidden_layer_sizes=(w), random_state=1));\n",
    "                end \n",
    "                \n",
    "                err = eval(P, x_data, c_data, weights)\n",
    "                println(EPS, \" \", w)\n",
    "                push!(results, (EPS, w, err))\n",
    "            end\n",
    "        \n",
    "        elseif model == :dt\n",
    "            if !test\n",
    "                Local.init_models(P, DecisionTreeClassifier(class_weight=\"balanced\", min_samples_leaf = 5));\n",
    "            end\n",
    "            err, all_error = eval(P, x_data, c_data, weights)\n",
    "            push!(results, (EPS, err))\n",
    "            println(EPS)\n",
    "        elseif model == :knn \n",
    "            if !test\n",
    "                Local.init_models(P, KNeighborsClassifier(n_neighbors = 3))\n",
    "            end \n",
    "            err, all_error = eval(P, x_data, c_data, weights)\n",
    "            push!(results, (EPS, err))\n",
    "            println(EPS)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    return results, all_error\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n",
      "(42,)\n",
      "(79,)\n",
      "(14,)\n",
      "(48,)\n",
      "(40,)\n",
      "(99,)\n",
      "(90,)\n",
      "(86,)\n",
      "(47,)\n",
      "(49,)\n",
      "(18,)\n",
      "(86,)\n",
      "(23,)\n",
      "(79,)\n",
      "(50,)\n",
      "(77,)\n",
      "(11,)\n",
      "(14,)\n",
      "(47,)\n",
      "(112,)\n",
      "(86,)\n",
      "(9,)\n",
      "(5,)\n",
      "(86,)\n",
      "(35,)\n",
      "(48,)\n",
      "(65,)\n",
      "(122,)\n",
      "(39,)\n",
      "(23,)\n",
      "(84,)\n",
      "(42,)\n",
      "(122,)\n",
      "(33,)\n",
      "(41,)\n",
      "(54,)\n",
      "(58,)\n",
      "(96,)\n",
      "(72,)\n",
      "(134,)\n",
      "(22,)\n",
      "(31,)\n",
      "(42,)\n",
      "(20,)\n",
      "(100,)\n",
      "(17,)\n",
      "(54,)\n",
      "(32,)\n",
      "(31,)\n",
      "(82,)\n",
      "(158,)\n",
      "(27,)\n",
      "(79,)\n",
      "(24,)\n",
      "(31,)\n",
      "(86,)\n",
      "(69,)\n",
      "(24,)\n",
      "(13,)\n",
      "(129,)\n",
      "(136,)\n",
      "(38,)\n",
      "(122,)\n",
      "(76,)\n",
      "(50,)\n",
      "(30,)\n",
      "(35,)\n",
      "(79,)\n",
      "(117,)\n",
      "(210,)\n",
      "(43,)\n",
      "(27,)\n",
      "(109,)\n",
      "(134,)\n",
      "(18,)\n",
      "(79,)\n",
      "(23,)\n",
      "(84,)\n",
      "(21,)\n",
      "(118,)\n",
      "(170,)\n",
      "(50,)\n",
      "(9,)\n",
      "(41,)\n",
      "(101,)\n",
      "(117,)\n",
      "(121,)\n",
      "(18,)\n",
      "(113,)\n",
      "(203,)\n",
      "(13,)\n",
      "(86,)\n",
      "(42,)\n",
      "(35,)\n",
      "(107,)\n",
      "(42,)\n",
      "(119,)\n",
      "(12,)\n",
      "(9,)\n",
      "(50,)\n",
      "(99,)\n",
      "(180,)\n",
      "(109,)\n",
      "(156,)\n",
      "(48,)\n",
      "(86,)\n",
      "(20,)\n",
      "(23,)\n",
      "(23,)\n",
      "(60,)\n",
      "(10,)\n",
      "(32,)\n",
      "(7,)\n",
      "(116,)\n",
      "(178,)\n",
      "(13,)\n",
      "(9,)\n",
      "(41,)\n",
      "(81,)\n",
      "(11,)\n",
      "(37,)\n",
      "(79,)\n",
      "(27,)\n",
      "(47,)\n",
      "(42,)\n",
      "(86,)\n",
      "(99,)\n",
      "(71,)\n",
      "(86,)\n",
      "(41,)\n",
      "(52,)\n",
      "(30,)\n",
      "(76,)\n",
      "(31,)\n",
      "(10,)\n",
      "(90,)\n",
      "(72,)\n",
      "(86,)\n",
      "(33,)\n",
      "(84,)\n",
      "(84,)\n",
      "(81,)\n",
      "(28,)\n",
      "(27,)\n",
      "(128,)\n",
      "(22,)\n",
      "(60,)\n",
      "(37,)\n",
      "(79,)\n",
      "(40,)\n",
      "(59,)\n",
      "(23,)\n",
      "(86,)\n",
      "(129,)\n",
      "(84,)\n",
      "(52,)\n",
      "(54,)\n",
      "(79,)\n",
      "(49,)\n",
      "(27,)\n",
      "(45,)\n",
      "(42,)\n",
      "(42,)\n",
      "(84,)\n",
      "(54,)\n",
      "(42,)\n",
      "(37,)\n",
      "(88,)\n",
      "(27,)\n",
      "(86,)\n",
      "(19,)\n",
      "(28,)\n",
      "(78,)\n",
      "(102,)\n",
      "(117,)\n",
      "(46,)\n",
      "(22,)\n",
      "(17,)\n",
      "(144,)\n",
      "(37,)\n",
      "(20,)\n",
      "(83,)\n",
      "(36,)\n",
      "(29,)\n",
      "(50,)\n",
      "(61,)\n",
      "(84,)\n",
      "(86,)\n",
      "(13,)\n",
      "(72,)\n",
      "(32,)\n",
      "(99,)\n",
      "(201,)\n",
      "(41,)\n",
      "(31,)\n",
      "(92,)\n",
      "(229,)\n",
      "(77,)\n",
      "(50,)\n",
      "(27,)\n",
      "(84,)\n",
      "(222,)\n",
      "(47,)\n",
      "(71,)\n",
      "(16,)\n",
      "(16,)\n",
      "(14,)\n",
      "(102,)\n",
      "(27,)\n",
      "(30,)\n",
      "(16,)\n",
      "(37,)\n",
      "(60,)\n",
      "(7,)\n",
      "(58,)\n",
      "(65,)\n",
      "(23,)\n",
      "(59,)\n",
      "(78,)\n",
      "(48,)\n",
      "(21,)\n",
      "(33,)\n",
      "(116,)\n",
      "(42,)\n",
      "(29,)\n",
      "(50,)\n",
      "(41,)\n",
      "(37,)\n",
      "(79,)\n",
      "(23,)\n",
      "(76,)\n",
      "(30,)\n",
      "(83,)\n",
      "(45,)\n",
      "(72,)\n",
      "(42,)\n",
      "(27,)\n",
      "(22,)\n",
      "(28,)\n",
      "(31,)\n",
      "(41,)\n",
      "(86,)\n",
      "(120,)\n",
      "(86,)\n",
      "(105,)\n",
      "(50,)\n",
      "(33,)\n",
      "(78,)\n",
      "(56,)\n",
      "(27,)\n",
      "(26,)\n",
      "(42,)\n",
      "(42,)\n",
      "(14,)\n",
      "(175,)\n",
      "(42,)\n",
      "(220,)\n",
      "(60,)\n",
      "(47,)\n",
      "(42,)\n",
      "(9,)\n",
      "(191,)\n",
      "(60,)\n",
      "(47,)\n",
      "(101,)\n",
      "(48,)\n",
      "(50,)\n",
      "(40,)\n",
      "(99,)\n",
      "(13,)\n",
      "(31,)\n",
      "(42,)\n",
      "(15,)\n",
      "(90,)\n",
      "(24,)\n",
      "(84,)\n",
      "(84,)\n",
      "(41,)\n",
      "(19,)\n",
      "(23,)\n",
      "(47,)\n",
      "(79,)\n",
      "(74,)\n",
      "(20,)\n",
      "(48,)\n",
      "(35,)\n",
      "(84,)\n",
      "(27,)\n",
      "(37,)\n",
      "(14,)\n",
      "(102,)\n",
      "(42,)\n",
      "(10,)\n",
      "(79,)\n",
      "(71,)\n",
      "(38,)\n",
      "(106,)\n",
      "(50,)\n",
      "(48,)\n",
      "(79,)\n",
      "(26,)\n",
      "(38,)\n",
      "(119,)\n",
      "(35,)\n",
      "(135,)\n",
      "(121,)\n",
      "(37,)\n",
      "(36,)\n",
      "(87,)\n",
      "(14,)\n",
      "(30,)\n",
      "(37,)\n",
      "(94,)\n",
      "(10,)\n",
      "(79,)\n",
      "(31,)\n",
      "(144,)\n",
      "(36,)\n",
      "(8,)\n",
      "(76,)\n",
      "(31,)\n",
      "(50,)\n",
      "(24,)\n",
      "(86,)\n",
      "(37,)\n",
      "(24,)\n",
      "(17,)\n",
      "(86,)\n",
      "(33,)\n",
      "(30,)\n",
      "(21,)\n",
      "(10,)\n",
      "(60,)\n",
      "(50,)\n",
      "(24,)\n",
      "(196,)\n",
      "(37,)\n",
      "(50,)\n",
      "(24,)\n",
      "(83,)\n",
      "(63,)\n",
      "(107,)\n",
      "(17,)\n",
      "(32,)\n",
      "(30,)\n",
      "(24,)\n",
      "(28,)\n",
      "(23,)\n",
      "(62,)\n",
      "(95,)\n",
      "(18,)\n",
      "(122,)\n",
      "(79,)\n",
      "(117,)\n",
      "(32,)\n",
      "(72,)\n",
      "(107,)\n",
      "(59,)\n",
      "(42,)\n",
      "(7,)\n",
      "(37,)\n",
      "(184,)\n",
      "(125,)\n",
      "(84,)\n",
      "(79,)\n",
      "(70,)\n",
      "(2,)\n",
      "(51,)\n",
      "(10,)\n",
      "(87,)\n",
      "(162,)\n",
      "(97,)\n",
      "(103,)\n",
      "(105,)\n",
      "(30,)\n",
      "(84,)\n",
      "(79,)\n",
      "(22,)\n",
      "(49,)\n",
      "(120,)\n",
      "(35,)\n",
      "(14,)\n",
      "(84,)\n",
      "(13,)\n",
      "(66,)\n",
      "(201,)\n",
      "(166,)\n",
      "(150,)\n",
      "(21,)\n",
      "(16,)\n",
      "(47,)\n",
      "(87,)\n",
      "(117,)\n",
      "(69,)\n",
      "(136,)\n",
      "(38,)\n",
      "(84,)\n",
      "(84,)\n",
      "(50,)\n",
      "(86,)\n",
      "(37,)\n",
      "(272,)\n",
      "(38,)\n",
      "(35,)\n",
      "(30,)\n",
      "(84,)\n",
      "(36,)\n",
      "(133,)\n",
      "(79,)\n",
      "(77,)\n",
      "(37,)\n",
      "(73,)\n",
      "(84,)\n",
      "(37,)\n",
      "(117,)\n",
      "(96,)\n",
      "(20,)\n",
      "(24,)\n",
      "(16,)\n",
      "(6,)\n",
      "(112,)\n",
      "(40,)\n",
      "(73,)\n",
      "(50,)\n",
      "(37,)\n",
      "(50,)\n",
      "(60,)\n",
      "(62,)\n",
      "(129,)\n",
      "(15,)\n",
      "(208,)\n",
      "(11,)\n",
      "(84,)\n",
      "(124,)\n",
      "(30,)\n",
      "(27,)\n",
      "(91,)\n",
      "(41,)\n",
      "(43,)\n",
      "(79,)\n",
      "(84,)\n",
      "(42,)\n",
      "(102,)\n",
      "(42,)\n",
      "(50,)\n",
      "(72,)\n",
      "(127,)\n",
      "(99,)\n",
      "(50,)\n",
      "(121,)\n",
      "(47,)\n",
      "(43,)\n",
      "(35,)\n",
      "(69,)\n",
      "(28,)\n",
      "(42,)\n",
      "(79,)\n",
      "(76,)\n",
      "(51,)\n",
      "(53,)\n",
      "(42,)\n",
      "(41,)\n",
      "(20,)\n",
      "(17,)\n",
      "(56,)\n",
      "(50,)\n",
      "(128,)\n",
      "(48,)\n",
      "(105,)\n",
      "(30,)\n",
      "(124,)\n",
      "(36,)\n",
      "(88,)\n",
      "(27,)\n",
      "(113,)\n",
      "(116,)\n",
      "(21,)\n",
      "(22,)\n",
      "(38,)\n",
      "(146,)\n",
      "(27,)\n",
      "(83,)\n",
      "(20,)\n",
      "(39,)\n",
      "(32,)\n",
      "(83,)\n",
      "(40,)\n",
      "(53,)\n",
      "(86,)\n",
      "(83,)\n",
      "(122,)\n",
      "(57,)\n",
      "(54,)\n",
      "(18,)\n",
      "(27,)\n",
      "(116,)\n",
      "(42,)\n",
      "(79,)\n",
      "(80,)\n",
      "(87,)\n",
      "(84,)\n",
      "(228,)\n",
      "(68,)\n",
      "(32,)\n",
      "(86,)\n",
      "(71,)\n",
      "(35,)\n",
      "(16,)\n",
      "(37,)\n",
      "(180,)\n",
      "(65,)\n",
      "(79,)\n",
      "(122,)\n",
      "(163,)\n",
      "(36,)\n",
      "(23,)\n",
      "(141,)\n",
      "(25,)\n",
      "(43,)\n",
      "(246,)\n",
      "(112,)\n",
      "(70,)\n",
      "(31,)\n",
      "(24,)\n",
      "(47,)\n",
      "(79,)\n",
      "(119,)\n",
      "(31,)\n",
      "(178,)\n",
      "(32,)\n",
      "(44,)\n",
      "(86,)\n",
      "(13,)\n",
      "(31,)\n",
      "(19,)\n",
      "(122,)\n",
      "(114,)\n",
      "(106,)\n",
      "(86,)\n",
      "(36,)\n",
      "(42,)\n",
      "(79,)\n",
      "(35,)\n",
      "(21,)\n",
      "(42,)\n",
      "(17,)\n",
      "(19,)\n",
      "(29,)\n",
      "(50,)\n",
      "(17,)\n",
      "(47,)\n",
      "(20,)\n",
      "(31,)\n",
      "(37,)\n",
      "(24,)\n",
      "(27,)\n",
      "(201,)\n",
      "(33,)\n",
      "(79,)\n",
      "(55,)\n",
      "(83,)\n",
      "(59,)\n",
      "(35,)\n",
      "(54,)\n",
      "(17,)\n",
      "(58,)\n",
      "(65,)\n",
      "(139,)\n",
      "(60,)\n",
      "(92,)\n",
      "(70,)\n",
      "(36,)\n",
      "(24,)\n",
      "(27,)\n",
      "(86,)\n",
      "(36,)\n",
      "(84,)\n",
      "(20,)\n",
      "(86,)\n",
      "(24,)\n",
      "(27,)\n",
      "(193,)\n",
      "(31,)\n",
      "(84,)\n",
      "(23,)\n",
      "(91,)\n",
      "(38,)\n",
      "(103,)\n",
      "(23,)\n",
      "(73,)\n",
      "(35,)\n",
      "(23,)\n",
      "(39,)\n",
      "(67,)\n",
      "(14,)\n",
      "(10,)\n",
      "(20,)\n",
      "(135,)\n",
      "(42,)\n",
      "(104,)\n",
      "(70,)\n",
      "(37,)\n",
      "(61,)\n",
      "(13,)\n",
      "(86,)\n",
      "(37,)\n",
      "(37,)\n",
      "(52,)\n",
      "(28,)\n",
      "(120,)\n",
      "(129,)\n",
      "(133,)\n",
      "(12,)\n",
      "(38,)\n",
      "(116,)\n",
      "(96,)\n",
      "(47,)\n",
      "(54,)\n",
      "(84,)\n",
      "(42,)\n",
      "(110,)\n",
      "(84,)\n",
      "(41,)\n",
      "(18,)\n",
      "(55,)\n",
      "(69,)\n",
      "(22,)\n",
      "(251,)\n",
      "(105,)\n",
      "(129,)\n",
      "(34,)\n",
      "(42,)\n",
      "(162,)\n",
      "(79,)\n",
      "(23,)\n",
      "(136,)\n",
      "(19,)\n",
      "(27,)\n",
      "(27,)\n",
      "(52,)\n",
      "(10,)\n",
      "(10,)\n",
      "(60,)\n",
      "(141,)\n",
      "(51,)\n",
      "(79,)\n",
      "(42,)\n",
      "(18,)\n",
      "(79,)\n",
      "(64,)\n",
      "(31,)\n",
      "(99,)\n",
      "(78,)\n",
      "(79,)\n",
      "(23,)\n",
      "(157,)\n",
      "(41,)\n",
      "(50,)\n",
      "(38,)\n",
      "(96,)\n",
      "(50,)\n",
      "(50,)\n",
      "(50,)\n",
      "(78,)\n",
      "(48,)\n",
      "(13,)\n",
      "(54,)\n",
      "(27,)\n",
      "(73,)\n",
      "(121,)\n",
      "(79,)\n",
      "(135,)\n",
      "(9,)\n",
      "(100,)\n",
      "(35,)\n",
      "(36,)\n",
      "(41,)\n",
      "(24,)\n",
      "(11,)\n",
      "(73,)\n",
      "(24,)\n",
      "(23,)\n",
      "(166,)\n",
      "(28,)\n",
      "(30,)\n",
      "(131,)\n",
      "(186,)\n",
      "(63,)\n",
      "(43,)\n",
      "(37,)\n",
      "(23,)\n",
      "(53,)\n",
      "(42,)\n",
      "(65,)\n",
      "(68,)\n",
      "(86,)\n",
      "(41,)\n",
      "(12,)\n",
      "(23,)\n",
      "(23,)\n",
      "(36,)\n",
      "(18,)\n",
      "(86,)\n",
      "(41,)\n",
      "(24,)\n",
      "(84,)\n",
      "(6,)\n",
      "(159,)\n",
      "(86,)\n",
      "(28,)\n",
      "(27,)\n",
      "(79,)\n",
      "(37,)\n",
      "(73,)\n",
      "(7,)\n",
      "(240,)\n",
      "(17,)\n",
      "(41,)\n",
      "(163,)\n",
      "(23,)\n",
      "(57,)\n",
      "(66,)\n",
      "(60,)\n",
      "(20,)\n",
      "(78,)\n",
      "(79,)\n",
      "(57,)\n",
      "(12,)\n",
      "(63,)\n",
      "(34,)\n",
      "(63,)\n",
      "(86,)\n",
      "(24,)\n",
      "(27,)\n",
      "(79,)\n",
      "(127,)\n",
      "(65,)\n",
      "(42,)\n",
      "(23,)\n",
      "(42,)\n",
      "(27,)\n",
      "(21,)\n",
      "(186,)\n",
      "(17,)\n",
      "(36,)\n",
      "(13,)\n",
      "(64,)\n",
      "(30,)\n",
      "(27,)\n",
      "(34,)\n",
      "(38,)\n",
      "(41,)\n",
      "(22,)\n",
      "(20,)\n",
      "(25,)\n",
      "(30,)\n",
      "(23,)\n",
      "(18,)\n",
      "(163,)\n",
      "(41,)\n",
      "(32,)\n",
      "(76,)\n",
      "(42,)\n",
      "(42,)\n",
      "(24,)\n",
      "(99,)\n",
      "(8,)\n",
      "(11,)\n",
      "(27,)\n",
      "(72,)\n",
      "(13,)\n",
      "(68,)\n",
      "(79,)\n",
      "(28,)\n",
      "(213,)\n",
      "(27,)\n",
      "(41,)\n",
      "(91,)\n",
      "(22,)\n",
      "(30,)\n",
      "(79,)\n",
      "(64,)\n",
      "(75,)\n",
      "(95,)\n",
      "(62,)\n",
      "(69,)\n",
      "(36,)\n",
      "(79,)\n",
      "(41,)\n",
      "(53,)\n",
      "(201,)\n",
      "(50,)\n",
      "(47,)\n",
      "(38,)\n",
      "(42,)\n",
      "(37,)\n",
      "(18,)\n",
      "(11,)\n",
      "(136,)\n",
      "(27,)\n",
      "(33,)\n",
      "(112,)\n",
      "(162,)\n",
      "(80,)\n",
      "(37,)\n",
      "(23,)\n",
      "(148,)\n",
      "(79,)\n",
      "(92,)\n",
      "(47,)\n",
      "(40,)\n",
      "(24,)\n",
      "(8,)\n",
      "(11,)\n",
      "(83,)\n",
      "(23,)\n",
      "(32,)\n",
      "(16,)\n",
      "(62,)\n",
      "(133,)\n",
      "(51,)\n",
      "(86,)\n",
      "(38,)\n",
      "(33,)\n",
      "(79,)\n",
      "(84,)\n",
      "(50,)\n",
      "(100,)\n",
      "(183,)\n",
      "(16,)\n",
      "(44,)\n",
      "(50,)\n",
      "(69,)\n",
      "(79,)\n",
      "(42,)\n",
      "(84,)\n",
      "(36,)\n",
      "(27,)\n",
      "(30,)\n",
      "(112,)\n",
      "(94,)\n",
      "(72,)\n",
      "(23,)\n",
      "(31,)\n",
      "(51,)\n",
      "(50,)\n",
      "(32,)\n",
      "(128,)\n",
      "(119,)\n",
      "(92,)\n",
      "(71,)\n",
      "(8,)\n",
      "(97,)\n",
      "(50,)\n",
      "(84,)\n",
      "(141,)\n",
      "(163,)\n",
      "(31,)\n",
      "(49,)\n",
      "(32,)\n",
      "(30,)\n",
      "(42,)\n",
      "(38,)\n",
      "(42,)\n",
      "(50,)\n",
      "(49,)\n",
      "(69,)\n",
      "(102,)\n",
      "(32,)\n",
      "(38,)\n",
      "(37,)\n",
      "(19,)\n",
      "(105,)\n",
      "(101,)\n",
      "(190,)\n",
      "(113,)\n",
      "(38,)\n",
      "(27,)\n",
      "(31,)\n",
      "(84,)\n",
      "(24,)\n",
      "(84,)\n",
      "(42,)\n",
      "(84,)\n",
      "(136,)\n",
      "(117,)\n",
      "(33,)\n",
      "(122,)\n",
      "(40,)\n",
      "(27,)\n",
      "(43,)\n",
      "(70,)\n",
      "(44,)\n",
      "(84,)\n",
      "(53,)\n",
      "(76,)\n",
      "(60,)\n",
      "(76,)\n",
      "(64,)\n",
      "(42,)\n",
      "(20,)\n",
      "(23,)\n",
      "(216,)\n",
      "(13,)\n",
      "(91,)\n",
      "(136,)\n",
      "(141,)\n",
      "(18,)\n",
      "(23,)\n",
      "(96,)\n",
      "(51,)\n",
      "(36,)\n",
      "(83,)\n",
      "(171,)\n",
      "(41,)\n",
      "(17,)\n",
      "(50,)\n",
      "(79,)\n",
      "(32,)\n",
      "(14,)\n",
      "(83,)\n",
      "(50,)\n",
      "(36,)\n",
      "(30,)\n",
      "(70,)\n",
      "(24,)\n",
      "(23,)\n",
      "(83,)\n",
      "(78,)\n",
      "(64,)\n",
      "(60,)\n",
      "(180,)\n",
      "(41,)\n",
      "(53,)\n",
      "(37,)\n",
      "(24,)\n",
      "(72,)\n",
      "(25,)\n",
      "(23,)\n",
      "(42,)\n",
      "(79,)\n",
      "(79,)\n",
      "(37,)\n",
      "(18,)\n",
      "(69,)\n",
      "(25,)\n",
      "(30,)\n",
      "(134,)\n",
      "(29,)\n",
      "(79,)\n",
      "(133,)\n",
      "(50,)\n",
      "(100,)\n",
      "(46,)\n",
      "(35,)\n",
      "(170,)\n",
      "(32,)\n",
      "(59,)\n",
      "(70,)\n",
      "(38,)\n",
      "(27,)\n",
      "(43,)\n",
      "(18,)\n",
      "(128,)\n",
      "(136,)\n",
      "(27,)\n",
      "(30,)\n",
      "(46,)\n",
      "(30,)\n",
      "(24,)\n",
      "(60,)\n",
      "(42,)\n",
      "(17,)\n",
      "(115,)\n",
      "(84,)\n",
      "(79,)\n",
      "(100,)\n",
      "(86,)\n",
      "(134,)\n",
      "(31,)\n",
      "(30,)\n",
      "(136,)\n",
      "(14,)\n",
      "(97,)\n",
      "(69,)\n",
      "(55,)\n",
      "(64,)\n",
      "(23,)\n",
      "(166,)\n",
      "(124,)\n",
      "(49,)\n",
      "(117,)\n",
      "(35,)\n",
      "(50,)\n",
      "(50,)\n",
      "(41,)\n",
      "(77,)\n",
      "(61,)\n",
      "(50,)\n",
      "(42,)\n",
      "(63,)\n",
      "(48,)\n",
      "(116,)\n",
      "(91,)\n",
      "(22,)\n",
      "(83,)\n",
      "(134,)\n",
      "(40,)\n",
      "(129,)\n",
      "(126,)\n",
      "(69,)\n",
      "(51,)\n",
      "(43,)\n",
      "(166,)\n",
      "(83,)\n",
      "(37,)\n",
      "(93,)\n",
      "(86,)\n",
      "(30,)\n",
      "(32,)\n",
      "(37,)\n",
      "(86,)\n",
      "(10,)\n",
      "(64,)\n",
      "(16,)\n",
      "(83,)\n",
      "(213,)\n",
      "(40,)\n",
      "(42,)\n",
      "(31,)\n",
      "(112,)\n",
      "(67,)\n",
      "(17,)\n",
      "(42,)\n",
      "(79,)\n",
      "(69,)\n",
      "(27,)\n",
      "(45,)\n",
      "(84,)\n",
      "(30,)\n",
      "(78,)\n",
      "(21,)\n",
      "(10,)\n",
      "(43,)\n",
      "(135,)\n",
      "(10,)\n",
      "(37,)\n",
      "(84,)\n",
      "(33,)\n",
      "(79,)\n",
      "(40,)\n",
      "(42,)\n",
      "(80,)\n",
      "(84,)\n",
      "(47,)\n",
      "(27,)\n",
      "(107,)\n",
      "(37,)\n",
      "(86,)\n",
      "(47,)\n",
      "(136,)\n",
      "(35,)\n",
      "(33,)\n",
      "(60,)\n",
      "(28,)\n",
      "(54,)\n",
      "(45,)\n",
      "(13,)\n",
      "(52,)\n",
      "(41,)\n",
      "(84,)\n",
      "(51,)\n",
      "(37,)\n",
      "(31,)\n",
      "(24,)\n",
      "(127,)\n",
      "(69,)\n",
      "(86,)\n",
      "(170,)\n",
      "(50,)\n",
      "(18,)\n",
      "(37,)\n",
      "(84,)\n",
      "(33,)\n",
      "(17,)\n",
      "(84,)\n",
      "(38,)\n",
      "(84,)\n",
      "(16,)\n",
      "(55,)\n",
      "(51,)\n",
      "(35,)\n",
      "(124,)\n",
      "(61,)\n",
      "(23,)\n",
      "(37,)\n",
      "(30,)\n",
      "(147,)\n",
      "(16,)\n",
      "(99,)\n",
      "(27,)\n",
      "(83,)\n",
      "(65,)\n",
      "(31,)\n",
      "(80,)\n",
      "(8,)\n",
      "(50,)\n",
      "(50,)\n",
      "(149,)\n",
      "(79,)\n",
      "(32,)\n",
      "(24,)\n",
      "(25,)\n",
      "(9,)\n",
      "(84,)\n",
      "(197,)\n",
      "(79,)\n",
      "(47,)\n",
      "(41,)\n",
      "(35,)\n",
      "(36,)\n",
      "(50,)\n",
      "(42,)\n",
      "(100,)\n",
      "(50,)\n",
      "(63,)\n",
      "(73,)\n",
      "(41,)\n",
      "(117,)\n",
      "(41,)\n",
      "(70,)\n",
      "(29,)\n",
      "(37,)\n",
      "(37,)\n",
      "(14,)\n",
      "(36,)\n",
      "(53,)\n",
      "(36,)\n",
      "(41,)\n",
      "(55,)\n",
      "(50,)\n",
      "(78,)\n",
      "(200,)\n",
      "(84,)\n",
      "(37,)\n",
      "(23,)\n",
      "(17,)\n",
      "(36,)\n",
      "(35,)\n",
      "(84,)\n",
      "(53,)\n",
      "(50,)\n",
      "(47,)\n",
      "(27,)\n",
      "(24,)\n",
      "(86,)\n",
      "(33,)\n",
      "(84,)\n",
      "(49,)\n",
      "(93,)\n",
      "(37,)\n",
      "(16,)\n",
      "(18,)\n",
      "(30,)\n",
      "(74,)\n",
      "(23,)\n",
      "(86,)\n",
      "(36,)\n",
      "(94,)\n",
      "(37,)\n",
      "(18,)\n",
      "(38,)\n",
      "(35,)\n",
      "(40,)\n",
      "(21,)\n",
      "(166,)\n",
      "(122,)\n",
      "(57,)\n",
      "(79,)\n",
      "(16,)\n",
      "(118,)\n",
      "(28,)\n",
      "(180,)\n",
      "(41,)\n",
      "(8,)\n",
      "(77,)\n",
      "(109,)\n",
      "(35,)\n",
      "(28,)\n",
      "(53,)\n",
      "(91,)\n",
      "(25,)\n",
      "(40,)\n",
      "(54,)\n",
      "(1,)\n",
      "(33,)\n",
      "(27,)\n",
      "(62,)\n",
      "(33,)\n",
      "(272,)\n",
      "(103,)\n",
      "(42,)\n",
      "(50,)\n",
      "(29,)\n",
      "(27,)\n",
      "(173,)\n",
      "(32,)\n",
      "(163,)\n",
      "(122,)\n",
      "(24,)\n",
      "(18,)\n",
      "(78,)\n",
      "(54,)\n",
      "(86,)\n",
      "(32,)\n",
      "(30,)\n",
      "(35,)\n",
      "(105,)\n",
      "(118,)\n",
      "(69,)\n",
      "(99,)\n",
      "(84,)\n",
      "(18,)\n",
      "(47,)\n",
      "(84,)\n",
      "(28,)\n",
      "(37,)\n",
      "(84,)\n",
      "(84,)\n",
      "(217,)\n",
      "(70,)\n",
      "(67,)\n",
      "(34,)\n",
      "(11,)\n",
      "(37,)\n",
      "(72,)\n",
      "(50,)\n",
      "(23,)\n",
      "(136,)\n",
      "(45,)\n",
      "(64,)\n",
      "(17,)\n",
      "(122,)\n",
      "(93,)\n",
      "(33,)\n",
      "(35,)\n",
      "(12,)\n",
      "(50,)\n",
      "(50,)\n",
      "(24,)\n",
      "(136,)\n",
      "(48,)\n",
      "(8,)\n",
      "(253,)\n",
      "(55,)\n",
      "(25,)\n",
      "(61,)\n",
      "(162,)\n",
      "(79,)\n",
      "(128,)\n",
      "(79,)\n",
      "(65,)\n",
      "(27,)\n",
      "(22,)\n",
      "(47,)\n",
      "(70,)\n",
      "(84,)\n",
      "(18,)\n",
      "(21,)\n",
      "(23,)\n",
      "(23,)\n",
      "(24,)\n",
      "(47,)\n",
      "(50,)\n",
      "(23,)\n",
      "(14,)\n",
      "(50,)\n",
      "(84,)\n",
      "(171,)\n",
      "(14,)\n",
      "(65,)\n",
      "(86,)\n",
      "(47,)\n",
      "(42,)\n",
      "(114,)\n",
      "(27,)\n",
      "(79,)\n",
      "(30,)\n",
      "(42,)\n",
      "(157,)\n",
      "(57,)\n",
      "(73,)\n",
      "(180,)\n",
      "(84,)\n",
      "(24,)\n",
      "(156,)\n",
      "(70,)\n",
      "(47,)\n",
      "(30,)\n",
      "(89,)\n",
      "(23,)\n",
      "(141,)\n",
      "(59,)\n",
      "(18,)\n",
      "(31,)\n",
      "(164,)\n",
      "(91,)\n",
      "(42,)\n",
      "(91,)\n",
      "(24,)\n",
      "(35,)\n",
      "(22,)\n",
      "(72,)\n",
      "(77,)\n",
      "(37,)\n",
      "(43,)\n",
      "(92,)\n",
      "(43,)\n",
      "(175,)\n",
      "(46,)\n",
      "(48,)\n",
      "(18,)\n",
      "(65,)\n",
      "(51,)\n",
      "(41,)\n",
      "(130,)\n",
      "(32,)\n",
      "(31,)\n",
      "(79,)\n",
      "(13,)\n",
      "(47,)\n",
      "(37,)\n",
      "(122,)\n",
      "(86,)\n",
      "(136,)\n",
      "(70,)\n",
      "(42,)\n",
      "(42,)\n",
      "(86,)\n",
      "(102,)\n",
      "(142,)\n",
      "(64,)\n",
      "(50,)\n",
      "(41,)\n",
      "(112,)\n",
      "(59,)\n",
      "(121,)\n",
      "(45,)\n",
      "(89,)\n",
      "(84,)\n",
      "(36,)\n",
      "(79,)\n",
      "(86,)\n",
      "(38,)\n",
      "(41,)\n",
      "(69,)\n",
      "(98,)\n",
      "(37,)\n",
      "(189,)\n",
      "(122,)\n",
      "(84,)\n",
      "(86,)\n",
      "(51,)\n",
      "(54,)\n",
      "(27,)\n",
      "(38,)\n",
      "(41,)\n",
      "(15,)\n",
      "(50,)\n",
      "(37,)\n",
      "(47,)\n",
      "(49,)\n",
      "(23,)\n",
      "(22,)\n",
      "(79,)\n",
      "(112,)\n",
      "(36,)\n",
      "(24,)\n",
      "(86,)\n",
      "(86,)\n",
      "(32,)\n",
      "(89,)\n",
      "(65,)\n",
      "(43,)\n",
      "(84,)\n",
      "(33,)\n",
      "(47,)\n",
      "(42,)\n",
      "(27,)\n",
      "(78,)\n",
      "(313,)\n",
      "(43,)\n",
      "(36,)\n",
      "(26,)\n",
      "(18,)\n",
      "(18,)\n",
      "(84,)\n",
      "(79,)\n",
      "(86,)\n",
      "(84,)\n",
      "(24,)\n",
      "(28,)\n",
      "(44,)\n",
      "(116,)\n",
      "(53,)\n",
      "(42,)\n",
      "(24,)\n",
      "(24,)\n",
      "(47,)\n",
      "(16,)\n",
      "(23,)\n",
      "(79,)\n",
      "(59,)\n",
      "(86,)\n",
      "(68,)\n",
      "(13,)\n",
      "(149,)\n",
      "(79,)\n",
      "(37,)\n",
      "(117,)\n",
      "(17,)\n",
      "(11,)\n",
      "(113,)\n",
      "(41,)\n",
      "(35,)\n",
      "(48,)\n",
      "(107,)\n",
      "(99,)\n",
      "(63,)\n",
      "(136,)\n",
      "(135,)\n",
      "(53,)\n",
      "(14,)\n",
      "(113,)\n",
      "(136,)\n",
      "(55,)\n",
      "(171,)\n",
      "(69,)\n",
      "(21,)\n",
      "(42,)\n",
      "(27,)\n",
      "(17,)\n",
      "(11,)\n",
      "(32,)\n",
      "(116,)\n",
      "(38,)\n",
      "(42,)\n",
      "(91,)\n",
      "(145,)\n",
      "(68,)\n",
      "(76,)\n",
      "(47,)\n",
      "(23,)\n",
      "(92,)\n",
      "(50,)\n",
      "(45,)\n",
      "(41,)\n",
      "(155,)\n",
      "(22,)\n",
      "(67,)\n",
      "(36,)\n",
      "(36,)\n",
      "(17,)\n",
      "(162,)\n",
      "(50,)\n",
      "(42,)\n",
      "(10,)\n",
      "(200,)\n",
      "(102,)\n",
      "(16,)\n",
      "(84,)\n",
      "(134,)\n",
      "(86,)\n",
      "(13,)\n",
      "(133,)\n",
      "(24,)\n",
      "(24,)\n",
      "(68,)\n",
      "(76,)\n",
      "(19,)\n",
      "(34,)\n",
      "(79,)\n",
      "(69,)\n",
      "(86,)\n",
      "(116,)\n",
      "(32,)\n",
      "(104,)\n",
      "(19,)\n",
      "(144,)\n",
      "(91,)\n",
      "(73,)\n",
      "(20,)\n",
      "(35,)\n",
      "(79,)\n",
      "(100,)\n",
      "(42,)\n",
      "(23,)\n",
      "(41,)\n",
      "(47,)\n",
      "(63,)\n",
      "(34,)\n",
      "(46,)\n",
      "(29,)\n",
      "(123,)\n",
      "(77,)\n",
      "(103,)\n",
      "(60,)\n",
      "(10,)\n",
      "(69,)\n",
      "(90,)\n",
      "(23,)\n",
      "(50,)\n",
      "(112,)\n",
      "(105,)\n",
      "(41,)\n",
      "(40,)\n",
      "(35,)\n",
      "(50,)\n",
      "(24,)\n",
      "(155,)\n",
      "(37,)\n",
      "(79,)\n",
      "(111,)\n",
      "(83,)\n",
      "(76,)\n",
      "(22,)\n",
      "(31,)\n",
      "(55,)\n",
      "(81,)\n",
      "(43,)\n",
      "(65,)\n",
      "(63,)\n",
      "(70,)\n",
      "(136,)\n",
      "(86,)\n",
      "(21,)\n",
      "(91,)\n",
      "(17,)\n",
      "(133,)\n",
      "(50,)\n",
      "(84,)\n",
      "(16,)\n",
      "(33,)\n",
      "(50,)\n",
      "(170,)\n",
      "(27,)\n",
      "(83,)\n",
      "(23,)\n",
      "(28,)\n",
      "(84,)\n",
      "(24,)\n",
      "(35,)\n",
      "(101,)\n",
      "(137,)\n",
      "(19,)\n",
      "(24,)\n",
      "(9,)\n",
      "(65,)\n",
      "(44,)\n",
      "(30,)\n",
      "(87,)\n",
      "(44,)\n",
      "(67,)\n",
      "(11,)\n",
      "(18,)\n",
      "(76,)\n",
      "(66,)\n",
      "(31,)\n",
      "(117,)\n",
      "(9,)\n",
      "(47,)\n",
      "(42,)\n",
      "(47,)\n",
      "(66,)\n",
      "(53,)\n",
      "(37,)\n",
      "(23,)\n",
      "(90,)\n",
      "(83,)\n",
      "(86,)\n",
      "(19,)\n",
      "(24,)\n",
      "(13,)\n",
      "(84,)\n",
      "(76,)\n",
      "(37,)\n",
      "(86,)\n",
      "(157,)\n",
      "(99,)\n",
      "(122,)\n",
      "(130,)\n",
      "(123,)\n",
      "(81,)\n",
      "(42,)\n",
      "(31,)\n",
      "(54,)\n",
      "(86,)\n",
      "(248,)\n",
      "(35,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022176447593442238\n",
      "0.023995317887873152\n",
      "0.03510348352381873\n",
      "0.029318535197265842\n",
      "0.03874887453454083\n",
      "0.04301010880277375\n",
      "0.04158356699957309\n",
      "0.045765787550728264\n",
      "0.04337504605444559\n",
      "0.046314773021586673\n",
      "0.05238620226271084\n",
      "0.05196607147412717\n",
      "0.052727342933003345\n",
      "0.05121433663593253\n",
      "0.05213844749628537\n",
      "0.055770254411311504\n",
      "0.054888595169641674\n",
      "0.05481127706813838\n",
      "0.05588331873324037\n",
      "0.05611463149517845\n",
      "0.05471945384877193\n",
      "0.055933246987178345\n",
      "0.05564529992685158\n",
      "0.056271937739932186\n",
      "0.05632214266073047\n",
      "0.05548461612054356\n",
      "0.054171540219130294\n",
      "0.05565748119318368\n",
      "0.056301631329580444\n",
      "0.05658652460113851\n",
      "0.060783899218363996\n",
      "0.06296651206951058\n",
      "0.06421678800279904\n",
      "0.06555982187680046\n",
      "0.06533235993232996\n",
      "0.06646577029781058\n",
      "0.06593322909222492\n",
      "0.0681754133309719\n",
      "0.06768600678655991\n",
      "0.06671470759366532\n",
      "0.06724434636083736\n",
      "0.06824352844538636\n",
      "0.06808743662375412\n",
      "0.06933797517782027\n",
      "0.06882146518661915\n",
      "0.06954450980720343\n",
      "0.06842338006388507\n",
      "0.06788500434069786\n",
      "0.06814804508280477\n",
      "0.06774724133413124\n",
      "0.06846525743880573\n",
      "0.06762335323128903\n",
      "0.06727159305067548\n",
      "0.06663174603282716\n",
      "0.06665316751681973\n",
      "0.06651722426387141\n",
      "0.0661988477607709\n",
      "0.06591852626996464\n",
      "0.06571491999523665\n",
      "0.06516734831177041\n",
      "0.0651743530616723\n",
      "0.06558057170181111\n",
      "0.06491231714998683\n",
      "0.06484946566833068\n",
      "0.06503839050528158\n",
      "0.0649230355513213\n",
      "0.06462350237704773\n",
      "0.0651185117264874\n",
      "0.06522554954492213\n",
      "0.06449857647602798\n",
      "0.06439029567448339\n",
      "0.06449238097154825\n",
      "0.06436898774502467\n",
      "0.06474436239871229\n",
      "0.064483527285489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Local.\n",
      "┌ Warning: `with_optimizer` is deprecated, replace `with_optimizer(Ipopt.Optimizer)` by `Ipopt.Optimizer`.\n",
      "│   caller = #with_optimizer#6(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Type, ::Gurobi.Env) at JuMP.jl:122\n",
      "└ @ JuMP /home/rares/.julia/packages/JuMP/Sp4sR/src/JuMP.jl:122\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] macro expansion at /home/rares/.julia/packages/MutableArithmetics/8xkW3/src/rewrite.jl:279 [inlined]",
      " [2] macro expansion at /home/rares/.julia/packages/JuMP/Sp4sR/src/macros.jl:380 [inlined]",
      " [3] #predict#6(::Bool, ::Int64, ::Function, ::Main.Local.Problem, ::Array{Float64,1}) at /home/rares/Desktop/spo/local.jl:172",
      " [4] #predict at ./none:0 [inlined]",
      " [5] eval(::Main.Local.Problem, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Array{Float64,1},1}) at ./In[49]:16",
      " [6] #cross_validate#151(::Symbol, ::Array{Int64,1}, ::Bool, ::Bool, ::Function, ::Main.Local.Problem, ::Float64) at ./In[128]:36",
      " [7] (::getfield(Main, Symbol(\"#kw##cross_validate\")))(::NamedTuple{(:widths, :model, :use_validation_data),Tuple{Array{Int64,1},Symbol,Bool}}, ::typeof(cross_validate), ::Main.Local.Problem, ::Float64) at ./none:0",
      " [8] top-level scope at In[152]:4"
     ]
    }
   ],
   "source": [
    "include(\"./local.jl\")\n",
    "import .Local\n",
    "P = Local.Problem(sp_oracle, objective_fun, X_train', c_train'; cap_type = :regret);\n",
    "\n",
    "results, all_errors = cross_validate(P, .02; widths = [(200)], \n",
    "                            model = :nn, use_validation_data = false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " (0.02, 100, (0.06508679556707685, [0.0, 0.0887985, 0.0336806, 0.0, 0.0, 0.0102814, 0.0101276, 0.0482479, 0.0709215, 0.0  …  0.176772, 0.0650192, 0.158778, 0.00209983, 0.0, 0.0319706, 0.0, 0.0, 0.144861, 0.121373]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `with_optimizer` is deprecated, replace `with_optimizer(Ipopt.Optimizer)` by `Ipopt.Optimizer`.\n",
      "│   caller = #with_optimizer#6(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::Type, ::Gurobi.Env) at JuMP.jl:122\n",
      "└ @ JuMP /home/rares/.julia/packages/JuMP/Sp4sR/src/JuMP.jl:122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14822517373076294\n",
      "0.09707015097526245\n",
      "0.09946152200205234\n",
      "0.08788084449762026\n",
      "0.07845316869599413\n",
      "0.07010649683481772\n",
      "0.07482148563372955\n",
      "0.09302459313773602\n",
      "0.0894086374584927\n",
      "0.08279486157003695\n",
      "0.07993116360984216\n",
      "0.07892455327675367\n",
      "0.07349327476318565\n",
      "0.07445331238117638\n",
      "0.0707050451329499\n",
      "0.06919778481413699\n",
      "0.07243564191932138\n",
      "0.0726750396835764\n",
      "0.0776379004252193\n",
      "0.09055215117804617\n",
      "0.08813686151707167\n",
      "0.09008746423062959\n",
      "0.09428974362973394\n",
      "0.09777739280500651\n",
      "0.0959418184541566\n",
      "0.09998308136146239\n",
      "0.09899306535208591\n",
      "0.09668140091382078\n",
      "0.09976447907105404\n",
      "0.10615240172842204\n",
      "0.10442935200705214\n",
      "0.10377334323139357\n",
      "0.10589296370257435\n",
      "0.10486750948911977\n",
      "0.1025789962233002\n",
      "0.10069452424013692\n",
      "0.10461131801417349\n",
      "0.10671156957943766\n",
      "0.10721370185608069\n",
      "0.10725748073483159\n",
      "0.10738405019215154\n",
      "0.1068731474493141\n",
      "0.11103202890649085\n",
      "0.10890537668731587\n",
      "0.10805207995156708\n",
      "0.10841393534689503\n",
      "0.10858153489157224\n",
      "0.11167690422718311\n",
      "0.11010567794745385\n",
      "0.10899134407129134\n",
      "0.10963738677319357\n",
      "0.10819841224853778\n",
      "0.1083432080004976\n",
      "0.1123142695449615\n",
      "0.11060705899229678\n",
      "0.10981183155377137\n",
      "0.1109090771167253\n",
      "0.11055512596508482\n",
      "0.10895400843713383\n",
      "0.10794391325968107\n",
      "0.10664612765015777\n",
      "0.10691294667586783\n",
      "0.11061544080819376\n",
      "0.10923570542936384\n",
      "0.110814518829065\n",
      "0.10921168948307165\n",
      "0.10848527066214968\n",
      "0.1103200098108767\n",
      "0.10994628206197073\n",
      "0.10882325257239134\n",
      "0.10871029593479854\n",
      "0.1085508847129182\n",
      "0.10848339042544877\n",
      "0.10851429183628707\n",
      "0.11281866791245478\n",
      "0.11335021185333218\n",
      "0.11505119952662655\n",
      "0.11560714551455901\n",
      "0.1200339358120955\n",
      "0.11928460181738143\n",
      "0.1183647346071266\n",
      "0.1186804026492363\n",
      "0.12095647291215021\n",
      "0.12088937383807863\n",
      "0.12148689416390117\n",
      "0.12016243315032614\n",
      "0.12002186689840093\n",
      "0.12137713579865336\n",
      "0.12024059752079955\n",
      "0.11900086283593002\n",
      "0.12103217235624565\n",
      "0.12003266140858414\n",
      "0.1198071125600125\n",
      "0.11926293656415449\n",
      "0.11880940621058872\n",
      "0.11924760769659314\n",
      "0.11916720528509334\n",
      "0.11881584743785915\n",
      "0.1192962006330447\n",
      "0.11996496429847048\n",
      "0.11915963056867576\n",
      "0.1181728521364994\n",
      "0.11793142150636944\n",
      "0.11689481754501757\n",
      "0.11642704585458097\n",
      "0.11550377999616877\n",
      "0.11554000645160861\n",
      "0.11469552301710803\n",
      "0.11673299511567925\n",
      "0.11650613008635703\n",
      "0.11930856191515346\n",
      "0.11878257918854854\n",
      "0.11870898546005774\n",
      "0.11850252759857638\n",
      "0.11842732103969948\n",
      "0.11876144890930403\n",
      "0.11822418435636982\n",
      "0.11780077408693687\n",
      "0.11692877156361217\n",
      "0.11691736231534824\n",
      "0.1164893102429473\n",
      "0.11583281961751214\n",
      "0.11711977012694334\n",
      "0.11642030448966738\n",
      "0.11641190953953559\n",
      "0.11644985127165274\n",
      "0.11613143931461352\n",
      "0.11818188358635795\n",
      "0.12075266848154223\n",
      "0.1201548659756814\n",
      "0.12019689643286549\n",
      "0.11992008346807397\n",
      "0.11943527437792988\n",
      "0.11883122199162977\n",
      "0.11885989255690856\n",
      "0.11833044779147826\n",
      "0.11777819455107369\n",
      "0.11792080079554225\n",
      "0.1173829075080054\n",
      "0.11699320005695019\n",
      "0.11637243285379938\n",
      "0.11714369386011182\n",
      "0.11717903486391264\n",
      "0.11670828661287591\n",
      "0.11640436212104548\n",
      "0.11659342741392523\n",
      "0.11752274123684724\n",
      "0.11730379945383437\n",
      "0.11737339914800797\n",
      "0.11672680542222395\n",
      "0.11788500928760345\n",
      "0.11750459637192129\n",
      "0.11729644777574212\n",
      "0.11688965812347817\n",
      "0.11664210627659453\n",
      "0.11608311507109921\n",
      "0.11710470528084672\n",
      "0.11700430704958018\n",
      "0.116841031508932\n",
      "0.11619787115980448\n",
      "0.11673495082546588\n",
      "0.11627469108580663\n",
      "0.11618462359269612\n",
      "0.1169176354057088\n",
      "0.11685135467965606\n",
      "0.11628640456801051\n",
      "0.11619233410453278\n",
      "0.11568301889018565\n",
      "0.11789059734331282\n",
      "0.11777617883747075\n",
      "0.1172799919356861\n",
      "0.11668368020934246\n",
      "0.11603483406279903\n",
      "0.1159051492577469\n",
      "0.11732140931620458\n",
      "0.11724723088075498\n",
      "0.11680741726349575\n",
      "0.1167152287332908\n",
      "0.1176937307900704\n",
      "0.11766309682206672\n",
      "0.11739014424969858\n",
      "0.11684147132953607\n",
      "0.11644077900826746\n",
      "0.11590285405482796\n",
      "0.11573981373149037\n",
      "0.11599194045851416\n",
      "0.11601468457848522\n",
      "0.11546177550689632\n",
      "0.11559074662503274\n",
      "0.11535089270680884\n",
      "0.11540033065799808\n",
      "0.11520515380349931\n",
      "0.11480471954319904\n",
      "0.11450883443699707\n",
      "0.11435626788548033\n",
      "0.11402270101697748\n",
      "0.11414361875039504\n",
      "0.11374552450975953\n",
      "0.11380461813486058\n",
      "0.11442343721069756\n",
      "0.11431320781570983\n",
      "0.11428805971184611\n",
      "0.11402511094027902\n",
      "0.11415300594076527\n",
      "0.1140351735893506\n",
      "0.1136669728930834\n",
      "0.11409696458532208\n",
      "0.11418799892812383\n",
      "0.11492939024365875\n",
      "0.11450889373380797\n",
      "0.11402375926795143\n",
      "0.1140777191155916\n",
      "0.1136109361111606\n",
      "0.1135759336951603\n",
      "0.11333924610124319\n",
      "0.11314827040474883\n",
      "0.113046679197873\n",
      "0.11294832656631935\n",
      "0.11496031051285252\n",
      "0.11471541201377447\n",
      "0.11457880364772431\n",
      "0.1144719076013178\n",
      "0.11615866493392658\n",
      "0.11758103991692012\n",
      "0.11796757430932052\n",
      "0.1175987857818641\n",
      "0.11752347202049189\n",
      "0.11741729917711094\n",
      "0.11830839149653166\n",
      "0.12106360447911549\n",
      "0.12088604813151331\n",
      "0.1211493221651385\n",
      "0.12117896513135812\n",
      "0.12157495808250172\n",
      "0.12199734134200343\n",
      "0.12165256398064085\n",
      "0.12171883860712966\n",
      "0.12139729211107562\n",
      "0.12138298631920647\n",
      "0.12153268406584589\n",
      "0.12108793559321457\n",
      "0.12070367485647546\n",
      "0.12115997338342975\n",
      "0.12266850770426384\n",
      "0.12281024125872882\n",
      "0.12333866539066728\n",
      "0.1236245106815227\n",
      "0.12321452953846394\n",
      "0.1231417120517432\n",
      "0.12280182689888754\n",
      "0.12235140867581015\n",
      "0.12223624546745152\n",
      "0.12274848848126658\n",
      "0.12245427226418405\n",
      "0.12216169269323922\n",
      "0.12188172425721411\n",
      "0.12145416728939878\n",
      "0.12130867165999917\n",
      "0.12098975733924158\n",
      "0.12107352426647792\n",
      "0.12062237135117691\n",
      "0.12069170081148958\n",
      "0.12123125847534423\n",
      "0.12104700366934082\n",
      "0.12098398775359129\n",
      "0.1217876302794391\n",
      "0.1217959099628925\n",
      "0.1213458510589358\n",
      "0.12099086388557642\n",
      "0.12154458953681958\n",
      "0.12111094885795114\n",
      "0.12077227557386948\n",
      "0.12047327684415739\n",
      "0.12016215899075369\n",
      "0.12170612504103466\n",
      "0.12183848097763739\n",
      "0.123014350270275\n",
      "0.12299567964269033\n",
      "0.12288023051583405\n",
      "0.12246246401971417\n",
      "0.12220664958500276\n",
      "0.12221960507145414\n",
      "0.12245433394240483\n",
      "0.12311595765139902\n",
      "0.12296830260254013\n",
      "0.12319593748191829\n",
      "0.12491390060417883\n",
      "0.12602353915157852\n",
      "0.12654701418097103\n",
      "0.12636655172440742\n",
      "0.12643795140664002\n",
      "0.1263018404168473\n",
      "0.12626060320509203\n",
      "0.12616990272641102\n",
      "0.1259973968992733\n",
      "0.12608366186655437\n",
      "0.12595171581241885\n",
      "0.12569348443972797\n",
      "0.12682551448933435\n",
      "0.12659133362153355\n",
      "0.12627871827551915\n",
      "0.12593699845825404\n",
      "0.12552136479997597\n",
      "0.125295630323073\n",
      "0.12495371171233159\n",
      "0.12496413867983647\n",
      "0.12501539897063174\n",
      "0.12479995340549795\n",
      "0.12449521580174037\n",
      "0.12419909918574834\n",
      "0.1242702734227616\n",
      "0.12415202464213139\n",
      "0.12379900179098884\n",
      "0.12404341977364484\n",
      "0.12455590079520626\n",
      "0.12454303159730402\n",
      "0.12491109755836964\n",
      "0.12465587985705502\n",
      "0.12474665824777645\n",
      "0.1251196626831108\n",
      "0.12522756598101267\n",
      "0.12489092101229018\n",
      "0.12469759385115133\n",
      "0.1250048498010616\n",
      "0.12468594855470745\n",
      "0.12440958046280697\n",
      "0.1242016406975121\n",
      "0.12411787678751618\n",
      "0.1246866403302623\n",
      "0.12467320794299318\n",
      "0.12467617106259953\n",
      "0.12453799640336086\n",
      "0.12472183402449272\n",
      "0.12468222524299784\n",
      "0.12469691906690776\n",
      "0.12445598141114957\n",
      "0.12427872927372932\n",
      "0.12431645001578322\n",
      "0.12443353438281197\n",
      "0.12498043437177568\n",
      "0.12472432147554684\n",
      "0.12438493830547864\n",
      "0.12410995861609594\n",
      "0.1238464714302512\n",
      "0.12353713738664857\n",
      "0.12352266931083039\n",
      "0.12382018976340509\n",
      "0.12371346586853565\n",
      "0.12365667137842506\n",
      "0.12357633422538279\n",
      "0.1232610082169537\n",
      "0.12311459867621678\n",
      "0.12300150653826332\n",
      "0.12288251258586086\n",
      "0.12292252709273029\n",
      "0.12281726407468387\n",
      "0.1227742165662279\n",
      "0.12268219839208644\n",
      "0.1244417218014401\n",
      "0.12510096089924913\n",
      "0.12485288171860379\n",
      "0.12473673604023057\n",
      "0.12476710222962765\n",
      "0.12454095231496566\n",
      "0.12427280532032876\n",
      "0.12429926816853323\n",
      "0.12493529072340726\n",
      "0.12464419515914368\n",
      "0.12485153941620847\n",
      "0.12481553229921437\n",
      "0.12449912020907833\n",
      "0.12526230480232337\n",
      "0.12541080713415645\n",
      "0.12529770273912424\n",
      "0.12559844530268843\n",
      "0.1253332479928003\n",
      "0.1253742551758014\n",
      "0.12507253529699536\n",
      "0.12493393125002557\n",
      "0.1246706112481462\n",
      "0.12539478114381936\n",
      "0.125143790618168\n",
      "0.12492345621006812\n",
      "0.12501367643204556\n",
      "0.12494434745906195\n",
      "0.12479492937210768\n",
      "0.12465991934377983\n",
      "0.12449747480575167\n",
      "0.12465282963108675\n",
      "0.124524437535461\n",
      "0.12483363326766593\n",
      "0.12477142529150144\n",
      "0.12450702740125262\n",
      "0.12451784041786772\n",
      "0.12493611796458907\n",
      "0.12471275622915069\n",
      "0.12494312866166386\n",
      "0.12495333677255627\n",
      "0.1261574955169534\n",
      "0.12614891438375925\n",
      "0.1261094374908093\n",
      "0.12579897577279123\n",
      "0.12587004275340147\n",
      "0.1257843733862568\n",
      "0.12603928391794694\n",
      "0.12606532119339536\n",
      "0.12585344344125776\n",
      "0.1257148762135225\n",
      "0.12562113163894345\n",
      "0.1258521188214928\n",
      "0.12580516076258433\n",
      "0.12565058901589643\n",
      "0.1255029372352421\n",
      "0.12557376161658165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1257303463196999\n",
      "0.12557986530719958\n",
      "0.12533708363718854\n",
      "0.12549179064142368\n",
      "0.12534894185413628\n",
      "0.12517233332541464\n",
      "0.12580049547382421\n",
      "0.12582703970056894\n",
      "0.12574166231513176\n",
      "0.12612064591952726\n",
      "0.12597280352090723\n",
      "0.12593321182555828\n",
      "0.1256613577213117\n",
      "0.1253781287075341\n",
      "0.12618046296524338\n",
      "0.1260346063921503\n",
      "0.12584712408131418\n",
      "0.12614077433271945\n",
      "0.12618182068108777\n",
      "0.12654717563322446\n",
      "0.12657181002721124\n",
      "0.1266218792786176\n",
      "0.12665357432340982\n",
      "0.12649682437258714\n",
      "0.12624413165803225\n",
      "0.12645749869809023\n",
      "0.12625700756184954\n",
      "0.12608004813651608\n",
      "0.12593564569548546\n",
      "0.12584423493432825\n",
      "0.1255811826987498\n",
      "0.1255243477471637\n",
      "0.1257214790904909\n",
      "0.12557698996518524\n",
      "0.12541094262013233\n",
      "0.1252247573057102\n",
      "0.12535899555846539\n",
      "0.12541676787508754\n",
      "0.12534842089839038\n",
      "0.12525290868460354\n",
      "0.12526864718853037\n",
      "0.12529338514060656\n",
      "0.1253156529873437\n",
      "0.1255264928341597\n",
      "0.125598814277673\n",
      "0.1253462337196049\n",
      "0.12533802263183352\n",
      "0.12514998840965214\n",
      "0.12501624128951028\n",
      "0.12476591985295195\n",
      "0.12493217263788763\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] sigatomic_end at ./c.jl:425 [inlined]",
      " [2] disable_sigint at ./c.jl:448 [inlined]",
      " [3] __pycall! at /home/rares/.julia/packages/PyCall/BcTLp/src/pyfncall.jl:42 [inlined]",
      " [4] _pycall!(::PyCall.PyObject, ::PyCall.PyObject, ::Tuple{Array{Array{Float64,1},1}}, ::Int64, ::Ptr{Nothing}) at /home/rares/.julia/packages/PyCall/BcTLp/src/pyfncall.jl:29",
      " [5] #call#117 at /home/rares/.julia/packages/PyCall/BcTLp/src/pyfncall.jl:11 [inlined]",
      " [6] (::PyCall.PyObject)(::Array{Array{Float64,1},1}) at /home/rares/.julia/packages/PyCall/BcTLp/src/pyfncall.jl:86",
      " [7] #predict#6(::Bool, ::Int64, ::Function, ::Main.Local.Problem, ::Array{Float64,1}) at /home/rares/Desktop/spo/local.jl:157",
      " [8] #predict at ./none:0 [inlined]",
      " [9] eval(::Main.Local.Problem, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Array{Float64,1},1}) at ./In[82]:16",
      " [10] top-level scope at In[83]:1"
     ]
    }
   ],
   "source": [
    "result, all_results = eval(P, X_test, c_test, true_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "println(sum(e > 0.1 ? 1 : 0 for e in all_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.059469194879824425, [0.0557208, 0.67841, 0.0397933, 0.0, 0.0, 0.0897497, 0.0, 0.0777506, 0.0, 0.00012772  …  0.0, 0.0404897, 0.0, 0.0, 0.0, 0.090384, 0.1108, 0.0, 0.0, 0.103424])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, al = eval_knn(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004731121818787011\n"
     ]
    }
   ],
   "source": [
    "println(quantile(al, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3854169056741408\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip000)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip001\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip000)\" d=\"\n",
       "M162.964 1486.45 L2352.76 1486.45 L2352.76 47.2441 L162.964 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip002\">\n",
       "    <rect x=\"162\" y=\"47\" width=\"2191\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  283.406,1486.45 283.406,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  673.188,1486.45 673.188,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1062.97,1486.45 1062.97,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1452.75,1486.45 1452.75,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1842.53,1486.45 1842.53,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2232.31,1486.45 2232.31,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  162.964,1445.72 2352.76,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  162.964,1144 2352.76,1144 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  162.964,842.276 2352.76,842.276 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  162.964,540.556 2352.76,540.556 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  162.964,238.836 2352.76,238.836 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  162.964,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  162.964,1486.45 162.964,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.406,1486.45 283.406,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  673.188,1486.45 673.188,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1062.97,1486.45 1062.97,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1452.75,1486.45 1452.75,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1842.53,1486.45 1842.53,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2232.31,1486.45 2232.31,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  162.964,1445.72 189.241,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  162.964,1144 189.241,1144 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  162.964,842.276 189.241,842.276 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  162.964,540.556 189.241,540.556 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  162.964,238.836 189.241,238.836 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip000)\" d=\"M 0 0 M283.406 1508.44 Q279.795 1508.44 277.967 1512 Q276.161 1515.55 276.161 1522.67 Q276.161 1529.78 277.967 1533.35 Q279.795 1536.89 283.406 1536.89 Q287.041 1536.89 288.846 1533.35 Q290.675 1529.78 290.675 1522.67 Q290.675 1515.55 288.846 1512 Q287.041 1508.44 283.406 1508.44 M283.406 1504.73 Q289.217 1504.73 292.272 1509.34 Q295.351 1513.92 295.351 1522.67 Q295.351 1531.4 292.272 1536.01 Q289.217 1540.59 283.406 1540.59 Q277.596 1540.59 274.518 1536.01 Q271.462 1531.4 271.462 1522.67 Q271.462 1513.92 274.518 1509.34 Q277.596 1504.73 283.406 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M663.57 1535.98 L671.209 1535.98 L671.209 1509.62 L662.899 1511.29 L662.899 1507.03 L671.162 1505.36 L675.838 1505.36 L675.838 1535.98 L683.477 1535.98 L683.477 1539.92 L663.57 1539.92 L663.57 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M1057.62 1535.98 L1073.94 1535.98 L1073.94 1539.92 L1052 1539.92 L1052 1535.98 Q1054.66 1533.23 1059.24 1528.6 Q1063.85 1523.95 1065.03 1522.61 Q1067.27 1520.08 1068.15 1518.35 Q1069.06 1516.59 1069.06 1514.9 Q1069.06 1512.14 1067.11 1510.41 Q1065.19 1508.67 1062.09 1508.67 Q1059.89 1508.67 1057.44 1509.43 Q1055.01 1510.2 1052.23 1511.75 L1052.23 1507.03 Q1055.05 1505.89 1057.51 1505.31 Q1059.96 1504.73 1062 1504.73 Q1067.37 1504.73 1070.56 1507.42 Q1073.76 1510.11 1073.76 1514.6 Q1073.76 1516.73 1072.95 1518.65 Q1072.16 1520.54 1070.05 1523.14 Q1069.47 1523.81 1066.37 1527.03 Q1063.27 1530.22 1057.62 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M1457 1521.29 Q1460.35 1522 1462.23 1524.27 Q1464.13 1526.54 1464.13 1529.87 Q1464.13 1534.99 1460.61 1537.79 Q1457.09 1540.59 1450.61 1540.59 Q1448.43 1540.59 1446.12 1540.15 Q1443.83 1539.73 1441.37 1538.88 L1441.37 1534.36 Q1443.32 1535.5 1445.63 1536.08 Q1447.95 1536.66 1450.47 1536.66 Q1454.87 1536.66 1457.16 1534.92 Q1459.48 1533.18 1459.48 1529.87 Q1459.48 1526.82 1457.32 1525.11 Q1455.19 1523.37 1451.37 1523.37 L1447.35 1523.37 L1447.35 1519.53 L1451.56 1519.53 Q1455.01 1519.53 1456.84 1518.16 Q1458.66 1516.77 1458.66 1514.18 Q1458.66 1511.52 1456.77 1510.11 Q1454.89 1508.67 1451.37 1508.67 Q1449.45 1508.67 1447.25 1509.09 Q1445.05 1509.5 1442.42 1510.38 L1442.42 1506.22 Q1445.08 1505.48 1447.39 1505.11 Q1449.73 1504.73 1451.79 1504.73 Q1457.11 1504.73 1460.22 1507.17 Q1463.32 1509.57 1463.32 1513.69 Q1463.32 1516.56 1461.67 1518.55 Q1460.03 1520.52 1457 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M1845.54 1509.43 L1833.74 1527.88 L1845.54 1527.88 L1845.54 1509.43 M1844.31 1505.36 L1850.19 1505.36 L1850.19 1527.88 L1855.12 1527.88 L1855.12 1531.77 L1850.19 1531.77 L1850.19 1539.92 L1845.54 1539.92 L1845.54 1531.77 L1829.94 1531.77 L1829.94 1527.26 L1844.31 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M2222.59 1505.36 L2240.95 1505.36 L2240.95 1509.3 L2226.87 1509.3 L2226.87 1517.77 Q2227.89 1517.42 2228.91 1517.26 Q2229.93 1517.07 2230.95 1517.07 Q2236.73 1517.07 2240.11 1520.24 Q2243.49 1523.42 2243.49 1528.83 Q2243.49 1534.41 2240.02 1537.51 Q2236.55 1540.59 2230.23 1540.59 Q2228.05 1540.59 2225.79 1540.22 Q2223.54 1539.85 2221.13 1539.11 L2221.13 1534.41 Q2223.22 1535.54 2225.44 1536.1 Q2227.66 1536.66 2230.14 1536.66 Q2234.14 1536.66 2236.48 1534.55 Q2238.82 1532.44 2238.82 1528.83 Q2238.82 1525.22 2236.48 1523.11 Q2234.14 1521.01 2230.14 1521.01 Q2228.26 1521.01 2226.39 1521.42 Q2224.54 1521.84 2222.59 1522.72 L2222.59 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M127.02 1431.51 Q123.408 1431.51 121.58 1435.08 Q119.774 1438.62 119.774 1445.75 Q119.774 1452.86 121.58 1456.42 Q123.408 1459.96 127.02 1459.96 Q130.654 1459.96 132.459 1456.42 Q134.288 1452.86 134.288 1445.75 Q134.288 1438.62 132.459 1435.08 Q130.654 1431.51 127.02 1431.51 M127.02 1427.81 Q132.83 1427.81 135.885 1432.42 Q138.964 1437 138.964 1445.75 Q138.964 1454.48 135.885 1459.08 Q132.83 1463.67 127.02 1463.67 Q121.209 1463.67 118.131 1459.08 Q115.075 1454.48 115.075 1445.75 Q115.075 1437 118.131 1432.42 Q121.209 1427.81 127.02 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M91.0475 1126.72 L109.404 1126.72 L109.404 1130.65 L95.3299 1130.65 L95.3299 1139.12 Q96.3484 1138.78 97.3669 1138.61 Q98.3855 1138.43 99.404 1138.43 Q105.191 1138.43 108.571 1141.6 Q111.95 1144.77 111.95 1150.19 Q111.95 1155.77 108.478 1158.87 Q105.006 1161.95 98.6864 1161.95 Q96.5105 1161.95 94.242 1161.58 Q91.9966 1161.21 89.5892 1160.47 L89.5892 1155.77 Q91.6725 1156.9 93.8947 1157.46 Q96.1169 1158.01 98.5938 1158.01 Q102.598 1158.01 104.936 1155.91 Q107.274 1153.8 107.274 1150.19 Q107.274 1146.58 104.936 1144.47 Q102.598 1142.36 98.5938 1142.36 Q96.7188 1142.36 94.8438 1142.78 Q92.992 1143.2 91.0475 1144.08 L91.0475 1126.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M127.02 1129.79 Q123.408 1129.79 121.58 1133.36 Q119.774 1136.9 119.774 1144.03 Q119.774 1151.14 121.58 1154.7 Q123.408 1158.24 127.02 1158.24 Q130.654 1158.24 132.459 1154.7 Q134.288 1151.14 134.288 1144.03 Q134.288 1136.9 132.459 1133.36 Q130.654 1129.79 127.02 1129.79 M127.02 1126.09 Q132.83 1126.09 135.885 1130.7 Q138.964 1135.28 138.964 1144.03 Q138.964 1152.76 135.885 1157.36 Q132.83 1161.95 127.02 1161.95 Q121.209 1161.95 118.131 1157.36 Q115.075 1152.76 115.075 1144.03 Q115.075 1135.28 118.131 1130.7 Q121.209 1126.09 127.02 1126.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M65.0292 855.621 L72.668 855.621 L72.668 829.255 L64.3579 830.922 L64.3579 826.663 L72.6217 824.996 L77.2976 824.996 L77.2976 855.621 L84.9365 855.621 L84.9365 859.556 L65.0292 859.556 L65.0292 855.621 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M100.006 828.075 Q96.3947 828.075 94.566 831.639 Q92.7605 835.181 92.7605 842.311 Q92.7605 849.417 94.566 852.982 Q96.3947 856.524 100.006 856.524 Q103.64 856.524 105.446 852.982 Q107.274 849.417 107.274 842.311 Q107.274 835.181 105.446 831.639 Q103.64 828.075 100.006 828.075 M100.006 824.371 Q105.816 824.371 108.872 828.977 Q111.95 833.561 111.95 842.311 Q111.95 851.037 108.872 855.644 Q105.816 860.227 100.006 860.227 Q94.1957 860.227 91.117 855.644 Q88.0614 851.037 88.0614 842.311 Q88.0614 833.561 91.117 828.977 Q94.1957 824.371 100.006 824.371 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M127.02 828.075 Q123.408 828.075 121.58 831.639 Q119.774 835.181 119.774 842.311 Q119.774 849.417 121.58 852.982 Q123.408 856.524 127.02 856.524 Q130.654 856.524 132.459 852.982 Q134.288 849.417 134.288 842.311 Q134.288 835.181 132.459 831.639 Q130.654 828.075 127.02 828.075 M127.02 824.371 Q132.83 824.371 135.885 828.977 Q138.964 833.561 138.964 842.311 Q138.964 851.037 135.885 855.644 Q132.83 860.227 127.02 860.227 Q121.209 860.227 118.131 855.644 Q115.075 851.037 115.075 842.311 Q115.075 833.561 118.131 828.977 Q121.209 824.371 127.02 824.371 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M66.0245 553.901 L73.6634 553.901 L73.6634 527.535 L65.3532 529.202 L65.3532 524.943 L73.6171 523.276 L78.293 523.276 L78.293 553.901 L85.9318 553.901 L85.9318 557.836 L66.0245 557.836 L66.0245 553.901 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M91.0475 523.276 L109.404 523.276 L109.404 527.211 L95.3299 527.211 L95.3299 535.683 Q96.3484 535.336 97.3669 535.174 Q98.3855 534.989 99.404 534.989 Q105.191 534.989 108.571 538.16 Q111.95 541.332 111.95 546.748 Q111.95 552.327 108.478 555.429 Q105.006 558.507 98.6864 558.507 Q96.5105 558.507 94.242 558.137 Q91.9966 557.767 89.5892 557.026 L89.5892 552.327 Q91.6725 553.461 93.8947 554.017 Q96.1169 554.572 98.5938 554.572 Q102.598 554.572 104.936 552.466 Q107.274 550.359 107.274 546.748 Q107.274 543.137 104.936 541.031 Q102.598 538.924 98.5938 538.924 Q96.7188 538.924 94.8438 539.341 Q92.992 539.757 91.0475 540.637 L91.0475 523.276 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M127.02 526.355 Q123.408 526.355 121.58 529.92 Q119.774 533.461 119.774 540.591 Q119.774 547.697 121.58 551.262 Q123.408 554.804 127.02 554.804 Q130.654 554.804 132.459 551.262 Q134.288 547.697 134.288 540.591 Q134.288 533.461 132.459 529.92 Q130.654 526.355 127.02 526.355 M127.02 522.651 Q132.83 522.651 135.885 527.258 Q138.964 531.841 138.964 540.591 Q138.964 549.318 135.885 553.924 Q132.83 558.507 127.02 558.507 Q121.209 558.507 118.131 553.924 Q115.075 549.318 115.075 540.591 Q115.075 531.841 118.131 527.258 Q121.209 522.651 127.02 522.651 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M68.6171 252.181 L84.9365 252.181 L84.9365 256.116 L62.9921 256.116 L62.9921 252.181 Q65.6541 249.426 70.2375 244.797 Q74.8439 240.144 76.0245 238.801 Q78.2698 236.278 79.1494 234.542 Q80.0522 232.783 80.0522 231.093 Q80.0522 228.339 78.1078 226.602 Q76.1865 224.866 73.0847 224.866 Q70.8856 224.866 68.4319 225.63 Q66.0014 226.394 63.2236 227.945 L63.2236 223.223 Q66.0477 222.089 68.5014 221.51 Q70.955 220.931 72.9921 220.931 Q78.3624 220.931 81.5568 223.616 Q84.7513 226.302 84.7513 230.792 Q84.7513 232.922 83.9411 234.843 Q83.1541 236.741 81.0476 239.334 Q80.4689 240.005 77.367 243.223 Q74.2652 246.417 68.6171 252.181 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M100.006 224.635 Q96.3947 224.635 94.566 228.2 Q92.7605 231.741 92.7605 238.871 Q92.7605 245.977 94.566 249.542 Q96.3947 253.084 100.006 253.084 Q103.64 253.084 105.446 249.542 Q107.274 245.977 107.274 238.871 Q107.274 231.741 105.446 228.2 Q103.64 224.635 100.006 224.635 M100.006 220.931 Q105.816 220.931 108.872 225.538 Q111.95 230.121 111.95 238.871 Q111.95 247.598 108.872 252.204 Q105.816 256.788 100.006 256.788 Q94.1957 256.788 91.117 252.204 Q88.0614 247.598 88.0614 238.871 Q88.0614 230.121 91.117 225.538 Q94.1957 220.931 100.006 220.931 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M127.02 224.635 Q123.408 224.635 121.58 228.2 Q119.774 231.741 119.774 238.871 Q119.774 245.977 121.58 249.542 Q123.408 253.084 127.02 253.084 Q130.654 253.084 132.459 249.542 Q134.288 245.977 134.288 238.871 Q134.288 231.741 132.459 228.2 Q130.654 224.635 127.02 224.635 M127.02 220.931 Q132.83 220.931 135.885 225.538 Q138.964 230.121 138.964 238.871 Q138.964 247.598 135.885 252.204 Q132.83 256.788 127.02 256.788 Q121.209 256.788 118.131 252.204 Q115.075 247.598 115.075 238.871 Q115.075 230.121 118.131 225.538 Q121.209 220.931 127.02 220.931 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip002)\" d=\"\n",
       "M283.406 142.286 L283.406 1445.72 L478.297 1445.72 L478.297 142.286 L283.406 142.286 L283.406 142.286  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.406,142.286 283.406,1445.72 478.297,1445.72 478.297,142.286 283.406,142.286 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M478.297 87.9763 L478.297 1445.72 L673.188 1445.72 L673.188 87.9763 L478.297 87.9763 L478.297 87.9763  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  478.297,87.9763 478.297,1445.72 673.188,1445.72 673.188,87.9763 478.297,87.9763 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M673.188 450.04 L673.188 1445.72 L868.079 1445.72 L868.079 450.04 L673.188 450.04 L673.188 450.04  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  673.188,450.04 673.188,1445.72 868.079,1445.72 868.079,450.04 673.188,450.04 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M868.079 691.416 L868.079 1445.72 L1062.97 1445.72 L1062.97 691.416 L868.079 691.416 L868.079 691.416  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  868.079,691.416 868.079,1445.72 1062.97,1445.72 1062.97,691.416 868.079,691.416 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M1062.97 968.998 L1062.97 1445.72 L1257.86 1445.72 L1257.86 968.998 L1062.97 968.998 L1062.97 968.998  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1062.97,968.998 1062.97,1445.72 1257.86,1445.72 1257.86,968.998 1062.97,968.998 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M1257.86 1137.96 L1257.86 1445.72 L1452.75 1445.72 L1452.75 1137.96 L1257.86 1137.96 L1257.86 1137.96  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1257.86,1137.96 1257.86,1445.72 1452.75,1445.72 1452.75,1137.96 1257.86,1137.96 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M1452.75 1264.68 L1452.75 1445.72 L1647.64 1445.72 L1647.64 1264.68 L1452.75 1264.68 L1452.75 1264.68  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1452.75,1264.68 1452.75,1445.72 1647.64,1445.72 1647.64,1264.68 1452.75,1264.68 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M1647.64 1318.99 L1647.64 1445.72 L1842.53 1445.72 L1842.53 1318.99 L1647.64 1318.99 L1647.64 1318.99  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1647.64,1318.99 1647.64,1445.72 1842.53,1445.72 1842.53,1318.99 1647.64,1318.99 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M1842.53 1282.79 L1842.53 1445.72 L2037.42 1445.72 L2037.42 1282.79 L1842.53 1282.79 L1842.53 1282.79  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1842.53,1282.79 1842.53,1445.72 2037.42,1445.72 2037.42,1282.79 1842.53,1282.79 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip002)\" d=\"\n",
       "M2037.42 1361.23 L2037.42 1445.72 L2232.31 1445.72 L2232.31 1361.23 L2037.42 1361.23 L2037.42 1361.23  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip002)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2037.42,1361.23 2037.42,1445.72 2232.31,1445.72 2232.31,1361.23 2037.42,1361.23 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip000)\" d=\"\n",
       "M1989.74 251.724 L2280.76 251.724 L2280.76 130.764 L1989.74 130.764  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1989.74,251.724 2280.76,251.724 2280.76,130.764 1989.74,130.764 1989.74,251.724 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip000)\" d=\"\n",
       "M2013.74 215.436 L2157.74 215.436 L2157.74 167.052 L2013.74 167.052 L2013.74 215.436  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip000)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2013.74,215.436 2157.74,215.436 2157.74,167.052 2013.74,167.052 2013.74,215.436 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip000)\" d=\"M 0 0 M2195.58 210.931 Q2193.77 215.561 2192.06 216.973 Q2190.35 218.385 2187.48 218.385 L2184.08 218.385 L2184.08 214.82 L2186.58 214.82 Q2188.33 214.82 2189.31 213.987 Q2190.28 213.154 2191.46 210.052 L2192.22 208.107 L2181.74 182.598 L2186.25 182.598 L2194.35 202.876 L2202.46 182.598 L2206.97 182.598 L2195.58 210.931 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip000)\" d=\"M 0 0 M2212.85 204.589 L2220.49 204.589 L2220.49 178.223 L2212.18 179.89 L2212.18 175.631 L2220.44 173.964 L2225.12 173.964 L2225.12 204.589 L2232.76 204.589 L2232.76 208.524 L2212.85 208.524 L2212.85 204.589 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = results[1][3][2]\n",
    "my_errors = Vector{Any}()\n",
    "for e in errors\n",
    "    if e < 5\n",
    "        push!(my_errors, e)\n",
    "    end\n",
    "end\n",
    "println(mean(my_errors))\n",
    "histogram(my_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Any,1}:\n",
       " (0.1, 30, (1.7899311152664583, [0.557973, 4.44885, 0.228306, 4.62438, 0.123549, 4.03626, 1.5439, 0.716159, 0.150416, 7.47809  …  10.5688, 8.39996, 0.918977, 0.717633, 0.99676, 10.4376, 0.224926, 3.14806, 4.01024, 2.24904]))\n",
       " (0.1, 70, (1.7210343149968694, [0.570788, 0.716843, 0.261485, 4.96483, 0.146831, 2.91744, 2.06295, 0.477852, 0.662672, 7.13032  …  4.51921, 7.12369, 0.517003, 0.766797, 2.8247, 8.63729, 1.05948, 3.38657, 4.72005, 3.34306]))\n",
       " (0.1, 100, (1.7549708238464046, [0.430561, 2.25288, 0.339484, 4.20612, 0.104167, 4.16838, 1.16274, 0.615239, 0.497601, 5.28446  …  2.83689, 2.17587, 2.77561, 0.722035, 1.4425, 8.97317, 0.828576, 3.10957, 4.21794, 1.91914]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, DecisionTree, Distributions, Gurobi, JuMP, LightGraphs, Parameters, SparseArrays, Statistics, ArgParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/rares/.julia/compiled/v1.0/Gadfly/DvECm.ji for Gadfly [c91e804a-d5a3-530f-b6f0-dfbca275c004]\n",
      "└ @ Base loading.jl:1190\n",
      "WARNING: using DecisionTree.DecisionTreeClassifier in module Main conflicts with an existing identifier.\n",
      "WARNING: using DecisionTree.RandomForestClassifier in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eval_spo (generic function with 1 method)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"originalspo/oracles/shortest_path_oracle.jl\")\n",
    "include(\"originalspo/solver/util.jl\")\n",
    "include(\"originalspo/solver/sgd.jl\")\n",
    "include(\"originalspo/solver/reformulation.jl\")\n",
    "include(\"originalspo/solver/random_forests_po.jl\")\n",
    "include(\"originalspo/solver/validation_set.jl\")\n",
    "include(\"originalspo/experiments/replication_functions.jl\")\n",
    "\n",
    "function eval_spo()\n",
    "    num_trials = 1\n",
    "\n",
    "    num_lambda = 10\n",
    "    lambda_max = 100\n",
    "    lambda_min_ratio = 10.0^(-8)\n",
    "    holdout_percent = 0.25\n",
    "    regularization = :lasso\n",
    "    different_validation_losses = false\n",
    "\n",
    "\n",
    "    # Fixed parameter sets (these are also the same for all experiments)\n",
    "    n_train_vec = [n_train]\n",
    "    polykernel_degree_vec = [1]\n",
    "    polykernel_noise_half_width_vec = [0]\n",
    "\n",
    "    # Set this to get reproducible results\n",
    "    rng_seed = 53\n",
    "\n",
    "\n",
    "    # Run experiment and get results\n",
    "    # Note that Gurobi enviornments are set within this function call\n",
    "    err, loss = shortest_path_multiple_replications(X_train, c_train, X_validation, c_validation, X_test, c_test, rng_seed, num_trials, grid_dim,\n",
    "        n_train_vec, n_test,\n",
    "        p_features, polykernel_degree_vec, polykernel_noise_half_width_vec;\n",
    "        num_lambda = num_lambda, lambda_max = lambda_max, lambda_min_ratio = lambda_min_ratio,\n",
    "        holdout_percent = holdout_percent, regularization = regularization,\n",
    "        different_validation_losses = different_validation_losses)\n",
    "\n",
    "    return err\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOISE: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Local.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05755878971335072\n",
      "0.061556121153382895\n",
      "0.058963559656866\n",
      "0.05702638865954108\n",
      "0.05887126927775786\n",
      "0.05815260909189963\n",
      "0.058493419135842016\n",
      "0.05801722531125279\n",
      "0.05846291745871854\n",
      "0.05805842663744129\n",
      "LOSS        : 0.05732002215469253\n",
      "MEAN   ERROR: 0.05805842663744129\n",
      "MEDIAN ERROR: 0.04881187448019433\n",
      "0.18\n",
      "0.05566150720301136\n",
      "0.061775141124069684\n",
      "0.06259557263878278\n",
      "0.06326873014775111\n",
      "0.0650795002820634\n",
      "0.06390799334080906\n",
      "0.06376148639685786\n",
      "0.0627879387552064\n",
      "0.06285908168104132\n",
      "0.06212375122784467\n",
      "LOSS        : 0.061191949053085094\n",
      "MEAN   ERROR: 0.06212375122784467\n",
      "MEDIAN ERROR: 0.05294932051255585\n",
      "0.2\n",
      "ERROR: Any[(0.18, 0.0580584), (0.2, 0.0621238)]\n",
      "NOISE: 0.75\n",
      "0.06068129730649323\n",
      "0.0689245381064108\n",
      "0.06721464087158024\n",
      "0.06668156210782346\n",
      "0.06480599414568192\n",
      "0.06499234281631369\n",
      "0.06590469860350652\n",
      "0.06489674047712939\n",
      "0.06392943370461622\n",
      "0.06420421368443718\n",
      "LOSS        : 0.06328801066519948\n",
      "MEAN   ERROR: 0.06420421368443718\n",
      "MEDIAN ERROR: 0.05486582000188235\n",
      "0.18\n",
      "0.05841263313172652\n",
      "0.0643646860456303\n",
      "0.06244456145997038\n",
      "0.06442450312179712\n",
      "0.06422239416101337\n",
      "0.06453789831438304\n",
      "0.06510460776814014\n",
      "0.06400977734944047\n",
      "0.06357572657436389\n",
      "0.06377151737169379\n",
      "LOSS        : 0.06294281859800371\n",
      "MEAN   ERROR: 0.06377151737169379\n",
      "MEDIAN ERROR: 0.05696841069161659\n",
      "0.2\n",
      "ERROR: Any[(0.18, 0.0642042), (0.2, 0.0637715)]\n",
      "NOISE: 0.8\n",
      "0.06570710243685239\n",
      "0.06691239556563168\n",
      "0.0656969564537101\n",
      "0.06587209627868992\n",
      "0.06466428270301328\n",
      "0.06647220288205484\n",
      "0.06605779273880454\n",
      "0.06586405543766273\n",
      "0.06610702672746253\n",
      "0.06587166765969903\n",
      "LOSS        : 0.06409805951144885\n",
      "MEAN   ERROR: 0.06587166765969903\n",
      "MEDIAN ERROR: 0.057404776053939746\n",
      "0.18\n",
      "0.06862868519952804\n",
      "0.06797752081111468\n",
      "0.06673549686676165\n",
      "0.06479659739328078\n",
      "0.06532860980025149\n",
      "0.06581844738317652\n",
      "0.06552026721915084\n",
      "0.06530835087747067\n",
      "0.0654354775378228\n",
      "0.06572521310359067\n",
      "LOSS        : 0.06376147862932714\n",
      "MEAN   ERROR: 0.06572521310359067\n",
      "MEDIAN ERROR: 0.058602659400077375\n",
      "0.2\n",
      "ERROR: Any[(0.18, 0.0658717), (0.2, 0.0657252)]\n",
      "NOISE: 0.85\n",
      "0.0627937660015004\n",
      "0.0656127978244695\n",
      "0.06617002698426191\n",
      "0.06670122386800532\n",
      "0.06707470893615432\n",
      "0.06681720106812716\n",
      "0.06732907328827294\n",
      "0.06685965664392685\n",
      "0.06827599609858069\n",
      "0.06836440928681652\n",
      "LOSS        : 0.06684101605218239\n",
      "MEAN   ERROR: 0.06836440928681652\n",
      "MEDIAN ERROR: 0.06005620587069725\n",
      "0.18\n",
      "0.06471067935610014\n",
      "0.06466379966146363\n",
      "0.06537542997449645\n",
      "0.06508172660649084\n",
      "0.06590102561428435\n",
      "0.06587619901533885\n",
      "0.06665647005740855\n",
      "0.06735163608869661\n",
      "0.06866379760373638\n",
      "0.06853920443291013\n",
      "LOSS        : 0.06649677936130491\n",
      "MEAN   ERROR: 0.06853920443291013\n",
      "MEDIAN ERROR: 0.059844661132133714\n",
      "0.2\n",
      "ERROR: Any[(0.18, 0.0683644), (0.2, 0.0685392)]\n",
      "NOISE: 0.9\n",
      "0.06739793411822201\n",
      "0.07580226091507862\n",
      "0.07234893602061794\n",
      "0.07009318370809395\n",
      "0.07165180429005345\n",
      "0.07125283894120672\n",
      "0.07093926280820115\n",
      "0.06967845958549132\n",
      "0.07052152744920483\n",
      "0.07141053573958589\n",
      "LOSS        : 0.06938899692783909\n",
      "MEAN   ERROR: 0.07141053573958589\n",
      "MEDIAN ERROR: 0.062028514497660066\n",
      "0.18\n",
      "0.06366227505596395\n",
      "0.06910783100628558\n",
      "0.07130195239100158\n",
      "0.06976060862909454\n",
      "0.07000022131269479\n",
      "0.0693754712034341\n",
      "0.06889697968347197\n",
      "0.06814219725416647\n",
      "0.06855158048484387\n",
      "0.06924314161551028\n",
      "LOSS        : 0.06709249393808968\n",
      "MEAN   ERROR: 0.06924314161551028\n",
      "MEDIAN ERROR: 0.057752272195001765\n",
      "0.2\n",
      "ERROR: Any[(0.18, 0.0714105), (0.2, 0.0692431)]\n",
      "NOISE: 0.95\n",
      "0.06565159129769521\n",
      "0.06972733819620272\n",
      "0.06909579521695114\n",
      "0.06935417754794479\n",
      "0.07168665347416404\n",
      "0.07066870627748068\n",
      "0.07064384744043437\n",
      "0.06956913381297936\n",
      "0.06927975861756251\n",
      "0.06987634639852455\n",
      "LOSS        : 0.06878734488939263\n",
      "MEAN   ERROR: 0.06987634639852455\n",
      "MEDIAN ERROR: 0.06420061952648033\n",
      "0.18\n",
      "0.06099720766391468\n",
      "0.06651076764323585\n",
      "0.06437308879840677\n",
      "0.0648381677051258\n",
      "0.06479248125174537\n",
      "0.06508359657861308\n",
      "0.06557692376918034\n",
      "0.06468423582578864\n",
      "0.06432042341789394\n",
      "0.0645701609352237\n",
      "LOSS        : 0.06394238816353465\n",
      "MEAN   ERROR: 0.0645701609352237\n",
      "MEDIAN ERROR: 0.05652121595177191\n",
      "0.2\n",
      "ERROR: Any[(0.18, 0.0698763), (0.2, 0.0645702)]\n",
      "NOISE: 1.0\n",
      "0.06510895857868843\n",
      "0.06991972851953139\n",
      "0.06715972508388021\n",
      "0.06712787725775865\n",
      "0.06869921207600263\n",
      "0.06923515149572777\n",
      "0.06875687188032807\n",
      "0.0692338941676071\n",
      "0.06943666349012789\n",
      "0.06980762873902845\n",
      "LOSS        : 0.06806670800754834\n",
      "MEAN   ERROR: 0.06980762873902845\n",
      "MEDIAN ERROR: 0.06083132831072571\n",
      "0.18\n",
      "0.061052502811312526\n",
      "0.06714910362117937\n",
      "0.06570292845175324\n",
      "0.06708967248572342\n",
      "0.068103154741411\n",
      "0.0686376410804953\n",
      "0.06826723321704783\n",
      "0.06906508029250297\n",
      "0.0691309535259943\n",
      "0.07041938367833166\n",
      "LOSS        : 0.06856948965485628\n",
      "MEAN   ERROR: 0.07041938367833166\n",
      "MEDIAN ERROR: 0.06056276348619865\n",
      "0.2\n",
      "ERROR: Any[(0.18, 0.0698076), (0.2, 0.0704194)]\n"
     ]
    }
   ],
   "source": [
    "include(\"./local.jl\")\n",
    "\n",
    "dt_results = Vector{Any}()\n",
    "all_dt_results = Vector{Any}()\n",
    "\n",
    "for noise in 0.7:0.05:1.0\n",
    "    println(\"NOISE: \", noise)\n",
    "    \n",
    "    X_train, c_train, X_validation, c_validation, X_test, c_test = \n",
    "        get_data(noise) \n",
    "    \n",
    "    P = Local.Problem(sp_oracle, objective_fun, X_train', c_train');\n",
    "    results, all_errors = cross_validate(P, 0.18:.02:.2; widths = [(100)], \n",
    "                                model = :dt, use_validation_data = false) \n",
    "    \n",
    "    println(\"ERROR: \", results)\n",
    "    push!(dt_results, results)\n",
    "    push!(all_dt_results, all_errors)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[Any[(0.18, 0.0580584), (0.2, 0.0621238)], Any[(0.18, 0.0642042), (0.2, 0.0637715)], Any[(0.18, 0.0658717), (0.2, 0.0657252)], Any[(0.18, 0.0683644), (0.2, 0.0685392)], Any[(0.18, 0.0714105), (0.2, 0.0692431)], Any[(0.18, 0.0698763), (0.2, 0.0645702)], Any[(0.18, 0.0698076), (0.2, 0.0704194)]]\n"
     ]
    }
   ],
   "source": [
    "println(dt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & 5.81 & 6.42 & 6.59 & 6.84 & 7.14 & 6.99 & 6.98 \\\\\n",
      "0.2 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & 6.21 & 6.38 & 6.57 & 6.85 & 6.92 & 6.46 & 7.04 \\\\\n",
      "0.22 & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & -- & --"
     ]
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 2-element Array{Any,1} at index [3]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 2-element Array{Any,1} at index [3]",
      "",
      "Stacktrace:",
      " [1] getindex(::Array{Any,1}, ::Int64) at ./array.jl:731",
      " [2] top-level scope at ./In[78]:14"
     ]
    }
   ],
   "source": [
    "# for i in 1:21\n",
    "#     print(round(0.05 * (i - 1), digits = 2))\n",
    "#     if i < 21 \n",
    "#         print(\" & \")\n",
    "#     end\n",
    "# end\n",
    "# println(\"\\\\\\\\\")\n",
    "\n",
    "for i in 1:6 \n",
    "    print(round(0.02 * (i - 1) + 0.18, digits=2))\n",
    "    j = 1\n",
    "    for k in 1:21\n",
    "        if 0.05 * (k - 1) > 0.69\n",
    "            print(\" & \", round(100 * dt_results[j][i][2], digits = 2))\n",
    "            j += 1\n",
    "        else \n",
    "            print(\" & --\")\n",
    "        end\n",
    "    end\n",
    "    print(\" \\\\\\\\\")\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(dt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10300112803107282, [0.197188, 0.146474, 0.249858, 0.018818, 0.0994762, 0.0, 0.0, 0.0317817, 0.0172509, 0.324613  …  0.0176765, 0.235934, 0.0, 0.0, 0.159974, 0.215098, 0.155502, 0.0750189, 0.116663, 0.37093])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors=10)\n",
    "neigh.fit(X_train', c_train')\n",
    "\n",
    "function knn(x)\n",
    "    return neigh.predict([x])[1,:]\n",
    "end\n",
    "\n",
    "eval_two_stage(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4437656953590096, [0.229412, 0.334899, 0.30848, 1.03017, 0.290837, 1.57212, 0.102883, 0.0167671, 0.298518, 1.07719  …  0.208202, 0.59773, 0.825292, 0.454429, 0.248584, 0.360849, 0.273566, 0.736864, 0.781927, 0.515038])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes = (100, 50))\n",
    "mlp.fit(X_train', c_train')\n",
    "\n",
    "function MLP(x)\n",
    "    return mlp.predict([x])[1,:]\n",
    "end\n",
    "\n",
    "eval_two_stage(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant LinearRegression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3981883412719595, [0.0, 0.397776, 0.181413, 0.146147, 0.543787, 0.307279, 0.184522, 0.11517, 0.155517, 0.223307  …  0.461812, 0.613084, 0.516178, 0.0345766, 0.0, 0.626909, 0.416853, 0.378836, 0.144861, 0.131236])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import linear_model: LinearRegression \n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train', c_train')\n",
    "\n",
    "function linear_mapping(x)\n",
    "    return linear.predict([x])[1,:]\n",
    "end\n",
    "\n",
    "eval_two_stage(linear_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3332599837777854, (21, 10))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_regression_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on to n_train = 200, polykernel_degree = 1, polykernel_noise_half_width = 0\n",
      "Current trial is 1\n",
      "Academic license - for non-commercial use only - expires 2021-07-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `Grid` is deprecated, use `grid` instead.\n",
      "│   caller = convert_grid_to_list at shortest_path_oracle.jl:95 [inlined]\n",
      "└ @ Core /home/rares/Desktop/spo/adamspo/oracles/shortest_path_oracle.jl:95\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: number of columns of each array must match (got (50, 25))",
     "output_type": "error",
     "traceback": [
      "ArgumentError: number of columns of each array must match (got (50, 25))",
      "",
      "Stacktrace:",
      " [1] _typed_vcat(::Type{Float64}, ::Tuple{Array{Float64,2},Array{Float64,2}}) at ./abstractarray.jl:1283",
      " [2] typed_vcat at ./abstractarray.jl:1297 [inlined]",
      " [3] vcat at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.0/SparseArrays/src/sparsevector.jl:1065 [inlined]",
      " [4] #shortest_path_replication#108(::Int64, ::Int64, ::Float64, ::Symbol, ::Bool, ::Bool, ::Function, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Int64, ::Int64, ::Int64, ::Int64, ::Int64, ::Int64, ::Int64) at /home/rares/Desktop/spo/originalspo/experiments/replication_functions.jl:123",
      " [5] #shortest_path_replication at ./none:0 [inlined]",
      " [6] #shortest_path_multiple_replications#109 at /home/rares/Desktop/spo/originalspo/experiments/replication_functions.jl:194 [inlined]",
      " [7] (::getfield(Main, Symbol(\"#kw##shortest_path_multiple_replications\")))(::NamedTuple{(:num_lambda, :lambda_max, :lambda_min_ratio, :holdout_percent, :regularization, :different_validation_losses),Tuple{Int64,Int64,Float64,Float64,Symbol,Bool}}, ::typeof(shortest_path_multiple_replications), ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float64,2}, ::Int64, ::Int64, ::Int64, ::Array{Int64,1}, ::Int64, ::Int64, ::Array{Int64,1}, ::Array{Int64,1}) at ./none:0",
      " [8] eval_spo() at ./In[52]:31",
      " [9] top-level scope at In[94]:1"
     ]
    }
   ],
   "source": [
    "eval_spo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
